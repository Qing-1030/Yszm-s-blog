{"config": {"lang": ["en"], "separator": "[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+", "pipeline": ["stemmer"], "fields": {"title": {"boost": 1000.0}, "text": {"boost": 1.0}, "tags": {"boost": 1000000.0}}}, "docs": [{"location": "", "title": "\u6b22\u8fce\u6765\u5230\u6211\u7684\u5b66\u4e60\u7a7a\u95f4", "text": "<p>\u8fd9\u91cc\u662f\u6211\u8bb0\u5f55 Python \u5b66\u4e60\u548c\u6280\u672f\u63a2\u7d22\u7684\u5730\u65b9\u3002</p>"}, {"location": "#_2", "title": "\u4eca\u65e5\u76ee\u6807", "text": "<p>\u8fd9\u662f\u4e8c\u7ea7\u6807\u9898\uff0c\u7528\u6765\u7ed9\u6587\u7ae0\u5206\u6bb5\u3002</p> <ul> <li> \u642d\u5efa\u535a\u5ba2\u73af\u5883</li> <li> \u5b66\u4e60 Markdown \u8bed\u6cd5</li> <li> \u5199\u4e0b\u7b2c\u4e00\u884c Python \u4ee3\u7801</li> </ul>"}, {"location": "#_3", "title": "\u6211\u7684\u7b2c\u4e00\u6bb5\u4ee3\u7801", "text": "<p>Markdown \u6700\u5f3a\u5927\u7684\u5730\u65b9\u5c31\u662f\u53ef\u4ee5\u9ad8\u4eae\u4ee3\u7801\uff0c\u770b\u4e0b\u9762\uff1a</p> <pre><code># \u8fd9\u662f\u4e00\u4e2a\u4ee3\u7801\u5757\uff0c\u7528\u4e09\u4e2a\u53cd\u5f15\u53f7\u5305\u88f9\ndef hello_blog():\n    print(\"Hello, World!\")\n\nhello_blog()\n</code></pre>"}, {"location": "build_blog/", "title": "\u6211\u662f\u5982\u4f55\u7528 Python \u642d\u5efa\u4e2a\u4eba\u535a\u5ba2\u7684", "text": "<p>\u8fd9\u662f\u6211\u535a\u5ba2\u7684\u7b2c\u4e00\u7bc7\u6b63\u5f0f\u6280\u672f\u6587\u7ae0\u3002\u5728\u8fd9\u7bc7\u6587\u7ae0\u91cc\uff0c\u6211\u60f3\u5206\u4eab\u6211\u662f\u5982\u4f55\u5229\u7528 Python \u548c\u5f00\u6e90\u5de5\u5177\uff0c\u5728\u96f6\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u642d\u5efa\u8d77\u8fd9\u4e2a\u72ec\u7acb\u535a\u5ba2\u7684\u3002</p>"}, {"location": "build_blog/#_1", "title": "\u4e3a\u4ec0\u4e48\u9009\u62e9\u201c\u6298\u817e\u201d\uff1f", "text": "<p>\u5176\u5b9e\u73b0\u5728\u7684\u5199\u4f5c\u5e73\u53f0\u5f88\u591a\uff08\u77e5\u4e4e\u3001\u516c\u4f17\u53f7\u7b49\uff09\uff0c\u4f46\u6211\u4f9d\u7136\u9009\u62e9\u81ea\u5df1\u642d\u5efa\u4e00\u4e2a\u535a\u5ba2\uff0c\u539f\u56e0\u5f88\u7b80\u5355\uff1a</p> <ol> <li>\u6570\u636e\u81ea\u4e3b\uff1a\u6240\u6709\u7684\u6587\u7ae0\u6e90\u7801\u90fd\u4fdd\u5b58\u5728\u6211\u81ea\u5df1\u7684 GitHub \u4ed3\u5e93\u91cc\uff0c\u4e0d\u4f1a\u56e0\u4e3a\u5e73\u53f0\u89c4\u5219\u53d8\u52a8\u800c\u4e22\u5931\u3002</li> <li>\u6781\u5ba2\u7cbe\u795e\uff1a\u4f5c\u4e3a\u4e00\u540d\u7f16\u7a0b\u5b66\u4e60\u8005\uff0c\u7528\u4ee3\u7801\u6784\u5efa\u81ea\u5df1\u7684\u201c\u540e\u82b1\u56ed\u201d\u672c\u8eab\u5c31\u662f\u4e00\u4ef6\u5f88\u6709\u6210\u5c31\u611f\u7684\u4e8b\u3002</li> <li>\u6280\u672f\u79ef\u7d2f\uff1a\u5728\u642d\u5efa\u8fc7\u7a0b\u4e2d\uff0c\u6211\u987a\u4fbf\u719f\u6089\u4e86 Python \u73af\u5883\u3001\u547d\u4ee4\u884c\u64cd\u4f5c\u548c Git \u7248\u672c\u63a7\u5236\u3002</li> </ol>"}, {"location": "build_blog/#_2", "title": "\u6211\u7684\u6280\u672f\u6808", "text": "<p>\u6211\u7684\u535a\u5ba2\u5e76\u4e0d\u662f\u4ece\u96f6\u5f00\u59cb\u5199 HTML/CSS\uff0c\u800c\u662f\u7ad9\u5728\u4e86\u5de8\u4eba\u7684\u80a9\u8180\u4e0a\u3002\u4ee5\u4e0b\u662f\u6211\u4f7f\u7528\u7684\u6838\u5fc3\u5de5\u5177\uff1a</p> \u5de5\u5177 \u4f5c\u7528 \u5907\u6ce8 Python \u57fa\u7840\u8bed\u8a00\u73af\u5883 \u5f3a\u5927\u7684\u751f\u6001\u652f\u6301 MkDocs \u9759\u6001\u7f51\u7ad9\u751f\u6210\u5668 \u628a Markdown \u8f6c\u6362\u6210\u7f51\u9875\u7684\u5f15\u64ce Material \u4e3b\u9898\u76ae\u80a4 \u63d0\u4f9b\u7f8e\u89c2\u7684 UI \u548c\u54cd\u5e94\u5f0f\u8bbe\u8ba1 GitHub Pages \u7f51\u7ad9\u6258\u7ba1 \u514d\u8d39\u7684\u4e91\u7aef\u670d\u52a1\u5668 PyCharm \u6211\u7684\u5f00\u53d1\u795e\u5668 \u7f16\u5199\u548c\u7ba1\u7406\u4ee3\u7801\u7684\u5730\u65b9"}, {"location": "build_blog/#_3", "title": "\u642d\u5efa\u8fc7\u7a0b\u590d\u76d8", "text": "<p>\u6574\u4e2a\u8fc7\u7a0b\u6bd4\u6211\u60f3\u8c61\u4e2d\u8981\u7b80\u5355\uff0c\u6838\u5fc3\u53ea\u6709\u4e09\u6b65\uff1a</p>"}, {"location": "build_blog/#1", "title": "1. \u5b89\u88c5\u201c\u5f15\u64ce\u201d", "text": "<p>\u5229\u7528 Python \u7684\u5305\u7ba1\u7406\u5de5\u5177 pip\uff0c\u5b89\u88c5 MkDocs \u548c Material \u4e3b\u9898\uff1a</p> <pre><code>pip install mkdocs-material\n</code></pre>"}, {"location": "build_blog/#2", "title": "2. \u521d\u59cb\u5316\u9879\u76ee", "text": "<p>\u4e00\u884c\u547d\u4ee4\u751f\u6210\u535a\u5ba2\u7684\u57fa\u7840\u9aa8\u67b6\uff1a</p> <pre><code>mkdocs new .\n</code></pre>"}, {"location": "build_blog/#3", "title": "3. \u914d\u7f6e\u4e0e\u53d1\u5e03", "text": "<p>\u5728 <code>mkdocs.yml</code> \u4e2d\u914d\u7f6e\u597d\u7f51\u7ad9\u540d\u79f0\u548c\u4e3b\u9898\u540e\uff0c\u4f7f\u7528 MkDocs \u63d0\u4f9b\u7684\u547d\u4ee4\u4e00\u952e\u90e8\u7f72\u5230 GitHub\uff1a</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>\u5c31\u662f\u8fd9\u4e48\u7b80\u5355\uff01\u5269\u4e0b\u7684\u5de5\u4f5c\uff0c\u5c31\u662f\u4e13\u6ce8\u4e8e\u7528 Markdown \u5199\u4f5c\u4e86\u3002</p>"}, {"location": "build_blog/#_4", "title": "\u9047\u5230\u7684\u5751\u4e0e\u89e3\u51b3", "text": "<p>\u5728\u642d\u5efa\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4e5f\u9047\u5230\u4e86\u4e00\u4e9b\u5c0f\u63d2\u66f2\uff08\u6bd4\u5982 Markdown \u4ee3\u7801\u5757\u7684\u8bed\u6cd5\u95ee\u9898\uff09\uff0c\u4f46\u901a\u8fc7\u67e5\u9605\u6587\u6863\u548c\u8c03\u8bd5\uff0c\u6700\u7ec8\u90fd\u89e3\u51b3\u4e86\u3002\u8fd9\u4e5f\u8ba9\u6211\u660e\u767d\uff1a\u7f16\u7a0b\u4e0d\u4ec5\u4ec5\u662f\u5199\u4ee3\u7801\uff0c\u66f4\u662f\u89e3\u51b3\u95ee\u9898\u7684\u8fc7\u7a0b\u3002</p>"}, {"location": "build_blog/#_5", "title": "\u5199\u5728\u6700\u540e", "text": "<p>\u8fd9\u4e2a\u535a\u5ba2\u76ee\u524d\u8fd8\u5f88\u7b80\u6d01\uff0c\u672a\u6765\u6211\u8ba1\u5212\uff1a</p> <ul> <li> \u8bb0\u5f55\u6211\u7684 C \u8bed\u8a00\u548c Python \u5b66\u4e60\u7b14\u8bb0</li> <li> \u5c1d\u8bd5\u4fee\u6539\u535a\u5ba2\u7684\u6837\u5f0f\uff08CSS\uff09</li> <li> \u575a\u6301\u6bcf\u5468\u66f4\u65b0</li> </ul> <p>\u5982\u679c\u4f60\u4e5f\u5bf9\u642d\u5efa\u535a\u5ba2\u611f\u5174\u8da3\uff0c\u6b22\u8fce\u4e0e\u6211\u4ea4\u6d41\uff01</p>"}, {"location": "guide/", "title": "\u535a\u5ba2\u65e5\u5e38\u7ef4\u62a4\u624b\u518c", "text": "<p>\u8fd9\u662f\u6211\u7684\u535a\u5ba2\u7ba1\u7406\u5907\u5fd8\u5f55\u3002\u4ee5\u540e\u6bcf\u6b21\u66f4\u65b0\u6587\u7ae0\uff0c\u6309\u7167\u8fd9\u4e2a\u6d41\u7a0b\u64cd\u4f5c\u5373\u53ef\u3002</p>"}, {"location": "guide/#4", "title": "\u4e00\u3001 \u65e5\u5e38\u66f4\u65b0\u6d41\u7a0b (4\u6b65\u6cd5)", "text": ""}, {"location": "guide/#1-write", "title": "\u7b2c 1 \u6b65\uff1a\u521b\u4f5c (Write)", "text": "<p>\u5728 <code>docs</code> \u6587\u4ef6\u5939\u4e0b\u65b0\u5efa <code>.md</code> \u6587\u4ef6\uff0c\u4f7f\u7528 Markdown \u8bed\u6cd5\u7f16\u5199\u5185\u5bb9\u3002</p>"}, {"location": "guide/#2-preview", "title": "\u7b2c 2 \u6b65\uff1a\u9884\u89c8 (Preview)", "text": "<p>\u5728 PyCharm \u7684 Terminal \u4e2d\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u542f\u52a8\u672c\u5730\u670d\u52a1\u5668\uff1a</p> <pre><code>mkdocs serve\n</code></pre> <ul> <li>\u6253\u5f00\u6d4f\u89c8\u5668\u8bbf\u95ee\uff1a[http://127.0.0.1:8000]</li> <li>\u68c0\u67e5\u6392\u7248\u548c\u5185\u5bb9\uff0c\u786e\u8ba4\u65e0\u8bef\u540e\u6309 <code>Ctrl + C</code> \u505c\u6b62\u670d\u52a1\u3002</li> </ul>"}, {"location": "guide/#3-deploy", "title": "\u7b2c 3 \u6b65\uff1a\u53d1\u5e03 (Deploy)", "text": "<p>\u5c06\u751f\u6210\u7684\u9759\u6001\u7f51\u9875\u63a8\u9001\u5230 GitHub Pages\uff0c\u8ba9\u7f51\u7ad9\u4e0a\u7ebf\uff1a</p> <pre><code>mkdocs gh-deploy\n</code></pre>"}, {"location": "guide/#4-backup", "title": "\u7b2c 4 \u6b65\uff1a\u5907\u4efd\u6e90\u7801 (Backup)", "text": "<p>\u91cd\u8981\uff01 \u8fd9\u4e00\u6b65\u662f\u4e3a\u4e86\u9632\u6b62\u6e90\u7801\u4e22\u5931\u3002\u5c06\u672c\u5730\u7684 Markdown \u539f\u7a3f\u540c\u6b65\u5230 GitHub \u4ed3\u5e93\uff1a</p> <pre><code>git add .\ngit commit -m \"\u65e5\u5e38\u66f4\u65b0\uff1a\u65b0\u589e\u6587\u7ae0\"\ngit push\n</code></pre>"}, {"location": "guide/#_2", "title": "\u4e8c\u3001 \u8fdb\u9636\u914d\u7f6e\uff1a\u7ba1\u7406\u83dc\u5355\u680f", "text": "<p>\u5982\u679c\u8981\u8c03\u6574\u6587\u7ae0\u5728\u5de6\u4fa7\u5bfc\u822a\u680f\u7684\u987a\u5e8f\uff0c\u9700\u8981\u4fee\u6539\u6839\u76ee\u5f55\u4e0b\u7684 <code>mkdocs.yml</code> \u6587\u4ef6\u3002</p> <p>\u627e\u5230 <code>nav</code> \u914d\u7f6e\u9879\uff08\u5982\u679c\u6ca1\u6709\u5c31\u81ea\u5df1\u52a0\u4e0a\uff09\uff1a</p> <pre><code>site_name: \u6211\u7684 Python \u5b66\u4e60\u7b14\u8bb0\ntheme:\n  name: material\n\n# \u5bfc\u822a\u680f\u914d\u7f6e\nnav:\n  - \u9996\u9875: index.md\n  - \u535a\u5ba2\u6307\u5357: guide.md  # &lt;--- \u8fd9\u5c31\u662f\u6211\u73b0\u5728\u8fd9\u7bc7\u6587\u7ae0\n  - Python\u7b14\u8bb0:\n      - \u53d8\u91cf\u4e0e\u7c7b\u578b: python_basic.md\n  - \u968f\u60f3: thoughts.md\n</code></pre> <p>\u6ce8\u610f\uff1a\u4fee\u6539 <code>mkdocs.yml</code> \u4e4b\u540e\uff0c\u901a\u5e38\u9700\u8981\u91cd\u542f <code>mkdocs serve</code> \u624d\u80fd\u770b\u5230\u6548\u679c\u3002</p>"}, {"location": "strategy/", "title": "\u6df1\u5ea6\u5b66\u4e60\u5b9e\u6218\u6307\u5357\uff1a\u5982\u4f55\u79d1\u5b66\u9009\u62e9\u6a21\u578b\u3001\u7b56\u7565\u4e0e\u589e\u5f3a\uff1f", "text": "<p>\u5728\u6df1\u5ea6\u5b66\u4e60\u9879\u76ee\u4e2d\uff0c\u7ecf\u5e38\u88ab\u620f\u79f0\u4e3a\u201c\u70bc\u4e39\u201d\uff0c\u56e0\u4e3a\u8d85\u53c2\u6570\u4f17\u591a\u3002\u4f46\u5b9e\u9645\u4e0a\uff0c\u4ece\u6a21\u578b\u9009\u62e9\u5230\u8bad\u7ec3\u7b56\u7565\uff0c\u90fd\u6709\u4e00\u5957\u6210\u719f\u7684\u65b9\u6cd5\u8bba\u3002\u672c\u6587\u5c06\u603b\u7ed3\u4e00\u5957\u901a\u7528\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u5e2e\u52a9\u4f60\u544a\u522b\u201c\u76f2\u731c\u201d\u3002</p>"}, {"location": "strategy/#1", "title": "1. \u6838\u5fc3\u51b3\u7b56\u903b\u8f91\uff1a\u4e09\u6b65\u8d70", "text": ""}, {"location": "strategy/#model-selection", "title": "\u7b2c\u4e00\u6b65\uff1a\u6a21\u578b\u9009\u62e9 (Model Selection)", "text": "<p>\u9009\u62e9\u6a21\u578b\u4e3b\u8981\u770b\u6570\u636e\u91cf\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u3002</p> \u573a\u666f \u7279\u5f81 \u63a8\u8350\u6a21\u578b \u8bad\u7ec3\u7b56\u7565 \u5c0f\u6570\u636e \u6bcf\u7c7b &lt; 100 \u5f20 ResNet18, MobileNet, ShuffleNet \u5fc5\u987b\u8fc1\u79fb\u5b66\u4e60 (Pretrained=True)\uff0c\u51bb\u7ed3\u90e8\u5206\u5c42 \u4e2d\u7b49\u6570\u636e \u6bcf\u7c7b 100-1000 \u5f20 ResNet34/50, EfficientNet-B0 \u8fc1\u79fb\u5b66\u4e60\uff0c\u5168\u53c2\u6570\u5fae\u8c03 (Full Fine-tuning) \u5927\u6570\u636e \u6bcf\u7c7b &gt; 1000 \u5f20 ResNet101, ViT, Swin Transformer \u53ef\u4ece\u5934\u8bad\u7ec3\uff0c\u6216\u5fae\u8c03\u5927\u6a21\u578b \u4f4e\u5206\u8fa8\u7387 32x32 (\u5982 CIFAR) CNN (ResNet, VGG) \u907f\u514d\u4f7f\u7528\u672a\u7ecf\u4f18\u5316\u7684 ViT \u9ad8\u5206\u8fa8\u7387 224x224 \u4ee5\u4e0a EfficientNet, Swin \u6a21\u578b\u8bbe\u8ba1\u5305\u542b\u4e86\u5bf9\u5927\u611f\u53d7\u91ce\u7684\u4f18\u5316"}, {"location": "strategy/#optimizer-scheduler", "title": "\u7b2c\u4e8c\u6b65\uff1a\u4f18\u5316\u5668\u4e0e\u8c03\u5ea6\u5668 (Optimizer &amp; Scheduler)", "text": "<p>\u5de5\u4e1a\u754c\u7684\u201c\u4e8c\u9009\u4e00\u201d\u6cd5\u5219\uff1a</p> <ul> <li>\u65b9\u6848 A\uff08\u7701\u5fc3\u9996\u9009\uff09\uff1aAdamW + CosineAnnealing</li> <li>\u7279\u70b9\uff1a\u6536\u655b\u5feb\uff0c\u5bf9\u8d85\u53c2\u4e0d\u654f\u611f\uff0c\u6548\u679c\u901a\u5e38\u5f88\u7a33\u3002</li> <li>\u9002\u7528\uff1a\u7edd\u5927\u591a\u6570\u4efb\u52a1\uff0cTransformer\uff0c\u4ee5\u53ca\u5feb\u901f\u9a8c\u8bc1\u60f3\u6cd5\u65f6\u3002</li> <li> <p>\u521d\u59cb LR\uff1a\u901a\u5e38\u8bbe\u4e3a <code>1e-3</code> \u6216 <code>1e-4</code>\u3002</p> </li> <li> <p>\u65b9\u6848 B\uff08\u5237\u699c\u8fdb\u9636\uff09\uff1aSGD + Momentum + ReduceLROnPlateau</p> </li> <li>\u7279\u70b9\uff1a\u6536\u655b\u6162\uff0c\u96be\u8c03\uff0c\u4f46\u8c03\u597d\u4e86\u6cdb\u5316\u6027\u80fd\u5f80\u5f80\u7565\u9ad8\u4e8e AdamW\u3002</li> <li>\u9002\u7528\uff1a\u5b66\u672f\u590d\u73b0\u3001\u7ade\u8d5b\u5237\u699c\u3001CNN \u67b6\u6784\u3002</li> <li>\u521d\u59cb LR\uff1a\u901a\u5e38\u8bbe\u4e3a <code>1e-1</code> \u6216 <code>1e-2</code>\u3002</li> </ul>"}, {"location": "strategy/#data-augmentation-strategy", "title": "\u7b2c\u4e09\u6b65\uff1a\u6570\u636e\u589e\u5f3a\u7b56\u7565 (Data Augmentation Strategy)", "text": "<p>\u589e\u5f3a\u5f3a\u5ea6\u5e94\u4e0e\u8fc7\u62df\u5408\u7a0b\u5ea6\u6210\u6b63\u6bd4\u3002</p> <ol> <li>\u8d77\u6b65\u671f\uff1a\u4ec5\u7528 <code>Resize</code> + <code>RandomHorizontalFlip</code>\u3002\u786e\u4fdd\u6a21\u578b\u80fd\u8dd1\u901a\uff0c\u89c2\u5bdf\u57fa\u51c6\u3002</li> <li>\u53d1\u73b0\u8fc7\u62df\u5408\uff08\u8bad\u7ec3\u96c6 &gt;&gt; \u9a8c\u8bc1\u96c6\uff09\uff1a\u52a0\u5165 <code>ColorJitter</code>\uff08\u989c\u8272\u6296\u52a8\uff09\u3001<code>RandomResizedCrop</code>\uff08\u968f\u673a\u88c1\u526a\uff09\u3002</li> <li>\u4e25\u91cd\u8fc7\u62df\u5408/\u51b2\u523a\u9ad8\u5206\uff1a\u52a0\u5165 <code>RandomErasing</code>\uff08\u64e6\u9664\uff09\u3001<code>Mixup</code>\u3001<code>CutMix</code> \u6216 <code>AutoAugment</code>\u3002</li> </ol>"}, {"location": "strategy/#2", "title": "2. \u6570\u636e\u589e\u5f3a\u53c2\u6570\u8be6\u89e3 (\u53c2\u6570\u901f\u67e5\u8868)", "text": "<p>\u5728\u4f7f\u7528 <code>torchvision.transforms</code> \u65f6\uff0c\u5404\u4e2a\u53c2\u6570\u7684\u5177\u4f53\u542b\u4e49\u5982\u4e0b\uff1a</p>"}, {"location": "strategy/#a", "title": "A. \u51e0\u4f55\u53d8\u6362\u7c7b", "text": "\u53d8\u6362\u65b9\u6cd5 \u5173\u952e\u53c2\u6570 \u8bf4\u660e \u63a8\u8350\u503c RandomResizedCrop <code>size</code> \u8f93\u51fa\u56fe\u50cf\u5c3a\u5bf8 <code>(224, 224)</code> \u6216 <code>(32, 32)</code> <code>scale</code> \u968f\u673a\u88c1\u526a\u9762\u79ef\u5360\u539f\u56fe\u7684\u6bd4\u4f8b <code>(0.08, 1.0)</code> (\u6807\u51c6) \u6216 <code>(0.6, 1.0)</code> (\u8f7b\u5ea6) <code>ratio</code> \u88c1\u526a\u533a\u57df\u7684\u957f\u5bbd\u6bd4 <code>(3/4, 4/3)</code> RandomHorizontalFlip <code>p</code> \u6267\u884c\u7ffb\u8f6c\u7684\u6982\u7387 <code>0.5</code> (\u5373\u4e00\u534a\u6982\u7387\u7ffb\u8f6c) RandomRotation <code>degrees</code> \u65cb\u8f6c\u89d2\u5ea6\u8303\u56f4 <code>15</code> (\u5373 -15\u00b0 \u5230 +15\u00b0) <code>fill</code> \u65cb\u8f6c\u540e\u7a7a\u767d\u533a\u57df\u586b\u5145\u8272 <code>0</code> (\u9ed1\u8272)"}, {"location": "strategy/#b", "title": "B. \u989c\u8272\u53d8\u6362\u7c7b", "text": "\u53d8\u6362\u65b9\u6cd5 \u5173\u952e\u53c2\u6570 \u8bf4\u660e \u63a8\u8350\u503c ColorJitter <code>brightness</code> \u4eae\u5ea6\u6296\u52a8\u56e0\u5b50 <code>0.4</code> <code>contrast</code> \u5bf9\u6bd4\u5ea6\u6296\u52a8\u56e0\u5b50 <code>0.4</code> <code>saturation</code> \u9971\u548c\u5ea6\u6296\u52a8\u56e0\u5b50 <code>0.4</code> <code>hue</code> \u8272\u76f8\u6296\u52a8\u56e0\u5b50 <code>0.1</code> (\u4e0d\u8981\u592a\u5927\uff0c\u5426\u5219\u989c\u8272\u4f1a\u5931\u771f) RandomGrayscale <code>p</code> \u8f6c\u4e3a\u7070\u5ea6\u56fe\u7684\u6982\u7387 <code>0.1</code>"}, {"location": "strategy/#c", "title": "C. \u906e\u6321\u4e0e\u64e6\u9664\u7c7b (\u6297\u8fc7\u62df\u5408\u795e\u5668)", "text": "\u53d8\u6362\u65b9\u6cd5 \u5173\u952e\u53c2\u6570 \u8bf4\u660e \u63a8\u8350\u503c RandomErasing <code>p</code> \u6267\u884c\u64e6\u9664\u7684\u6982\u7387 <code>0.2</code> \u5230 <code>0.5</code> (\u9700\u653e\u5728ToTensor\u4e4b\u540e) <code>scale</code> \u64e6\u9664\u533a\u57df\u9762\u79ef\u6bd4\u4f8b <code>(0.02, 0.33)</code> <code>ratio</code> \u64e6\u9664\u533a\u57df\u957f\u5bbd\u6bd4 <code>(0.3, 3.3)</code> <code>value</code> \u586b\u5145\u503c <code>'random'</code> (\u566a\u70b9) \u6216 <code>0</code> (\u9ed1\u5757)"}, {"location": "strategy/#3", "title": "3. \u5e38\u7528\u7684\u6570\u636e\u589e\u5f3a\u914d\u7f6e\u4ee3\u7801", "text": "<p>\u4ee5\u4e0b\u662f\u4e24\u5957\u62ff\u6765\u5373\u7528\u7684\u914d\u7f6e\uff0c\u76f4\u63a5\u590d\u5236\u5230\u4ee3\u7801\u4e2d\u5373\u53ef\u3002</p>"}, {"location": "strategy/#resnetimagenet", "title": "\u914d\u7f6e\u4e00\uff1a\u5de5\u4e1a\u7ea7\u6807\u51c6\u589e\u5f3a (ResNet/ImageNet \u6807\u51c6)", "text": "<p>\u9002\u7528\u4e8e\u5927\u591a\u6570 CNN \u8bad\u7ec3\u4efb\u52a1\uff0c\u517c\u987e\u591a\u6837\u6027\u548c\u7a33\u5b9a\u6027\u3002</p> <pre><code>from torchvision import transforms\n\n# \u5047\u8bbe\u8f93\u5165\u5c3a\u5bf8\ncrop_size = 224 # \u5982\u679c\u662fCIFAR\u6539\u4e3a32\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_tf_standard = transforms.Compose([\n    # 1. \u968f\u673a\u88c1\u526a\u5e76\u7f29\u653e (\u6700\u5f3a\u51e0\u4f55\u589e\u5f3a)\n    # scale=(0.08, 1.0) \u5141\u8bb8\u88c1\u526a\u51fa\u7269\u4f53\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u5f3a\u8feb\u6a21\u578b\u5b66\u4e60\u5c40\u90e8\u7279\u5f81\n    transforms.RandomResizedCrop((crop_size, crop_size), scale=(0.08, 1.0)),\n\n    # 2. \u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\n    transforms.RandomHorizontalFlip(p=0.5),\n\n    # 3. \u989c\u8272\u6296\u52a8 (\u4eae\u5ea6/\u5bf9\u6bd4\u5ea6/\u9971\u548c\u5ea6/\u8272\u76f8)\n    transforms.RandomApply([\n        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n    ], p=0.8),\n\n    # 4. \u8f6c Tensor \u5e76 \u5f52\u4e00\u5316\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n\n    # 5. [\u53ef\u9009] \u968f\u673a\u64e6\u9664 (\u5fc5\u987b\u5728 Normalize \u4e4b\u540e)\n    transforms.RandomErasing(p=0.25, value='random')\n])\n</code></pre>"}, {"location": "strategy/#sota-autoaugment", "title": "\u914d\u7f6e\u4e8c\uff1aSOTA \u61d2\u4eba\u589e\u5f3a (AutoAugment)", "text": "<p>\u5982\u679c\u4f60\u4e0d\u60f3\u624b\u52a8\u8c03\u53c2\uff0c\u76f4\u63a5\u7528 Google \u641c\u7d22\u51fa\u6765\u7684\u6700\u4f73\u7b56\u7565\u3002</p> <pre><code>from torchvision import transforms\n\ntrain_tf_auto = transforms.Compose([\n    # \u5148\u628a\u56fe\u7247\u53d8\u6210\u56fa\u5b9a\u5927\u5c0f\uff0c\u65b9\u4fbf AutoAugment \u5904\u7406\n    transforms.Resize((crop_size, crop_size)), \n\n    # \u81ea\u52a8\u589e\u5f3a\u7b56\u7565 (\u4f7f\u7528 ImageNet \u6216 CIFAR10 \u7b56\u7565)\n    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.IMAGENET),\n\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std),\n\n    # \u4f9d\u7136\u53ef\u4ee5\u53e0\u52a0 RandomErasing\n    transforms.RandomErasing(p=0.25)\n])\n</code></pre>"}, {"location": "strategy/#4", "title": "4. \u603b\u7ed3\uff1a\u4e07\u80fd\u8d77\u624b\u5f0f", "text": "<p>\u5982\u679c\u4f60\u9762\u5bf9\u4e00\u4e2a\u65b0\u4efb\u52a1\uff0c\u4e0d\u77e5\u9053\u4ece\u4f55\u4e0b\u624b\uff0c\u8bf7\u76f4\u63a5\u5957\u7528\u8fd9\u4e2a\u914d\u7f6e\uff1a</p> <ul> <li>\u6a21\u578b\uff1a<code>ResNet50</code> (Pretrained=True)</li> <li>\u4f18\u5316\u5668\uff1a<code>AdamW</code> (LR=1e-4)</li> <li>\u8c03\u5ea6\u5668\uff1a<code>CosineAnnealingLR</code></li> <li>\u589e\u5f3a\uff1a<code>RandomResizedCrop</code> + <code>HorizontalFlip</code> + <code>Normalize</code></li> <li>Batch Size\uff1a\u663e\u5b58\u5141\u8bb8\u7684\u6700\u5927\u503c</li> </ul> <p>\u5148\u8dd1\u901a\uff0c\u518d\u6839\u636e\u201c\u8bad\u7ec3\u96c6\u597d\u4e8e\u9a8c\u8bc1\u96c6 -&gt; \u8fc7\u62df\u5408\u201d\u7684\u539f\u5219\u53bb\u52a0\u589e\u5f3a\u6216\u6b63\u5219\u5316\u3002</p>"}, {"location": "cv/Compare_model/", "title": "\u5bf9\u6bd4\u6a21\u578b\u6027\u80fd", "text": "", "tags": ["Python", "\u6a21\u578b\u6027\u80fd"]}, {"location": "cv/Compare_model/#_2", "title": "\u6838\u5fc3\u4ee3\u7801\uff1a", "text": "<pre><code>import os\nimport time\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport timm\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport re\n\n# \u5c1d\u8bd5\u5bfc\u5165 FLOPs \u8ba1\u7b97\u5de5\u5177 (\u5982\u679c\u6ca1\u6709\u5b89\u88c5 thop\uff0c\u4f1a\u81ea\u52a8\u8df3\u8fc7)\ntry:\n    from thop import profile\n    HAS_THOP = True\nexcept ImportError:\n    HAS_THOP = False\n    print(\"[\u63d0\u793a] \u672a\u68c0\u6d4b\u5230 'thop' \u5e93\uff0c\u5c06\u8df3\u8fc7 FLOPs \u8ba1\u7b97\u3002(\u5efa\u8bae: pip install thop)\")\n\n# ==========================================\n# 1. \u5168\u5c40\u914d\u7f6e [\u6838\u5fc3\u4fee\u6539\u533a]\n# ==========================================\nclass Config:\n    # --- [\u5fc5\u586b] \u5f85\u5bf9\u6bd4\u7684\u6a21\u578b\u6e05\u5355 ---\n    # \u8bf7\u5728\u4e0b\u65b9\u5217\u8868\u4e2d\u586b\u5199\u4f60\u8981PK\u7684\u6a21\u578b\u4fe1\u606f\n    # \u683c\u5f0f: {'name': '\u81ea\u5b9a\u4e49\u663e\u793a\u540d', 'arch': '\u6a21\u578b\u67b6\u6784\u540d(\u5982resnet50)', 'path': '.pth\u6743\u91cd\u8def\u5f84'}\n    MY_MODELS = [\n        {\n            'name': 'ResNet50_Run1', \n            'arch': 'resnet50', \n            'path': './results/resnet50_run1/best_model.pth' \n        },\n        {\n            'name': 'ResNet50_Run2', \n            'arch': 'resnet50', \n            'path': './results/resnet50_run2/best_model.pth'\n        },\n        # {\n        #     'name': 'MobileNetV3', \n        #     'arch': 'mobilenetv3_large_100', \n        #     'path': './results/mobilenet/best_model.pth'\n        # }\n    ]\n\n    # --- [\u5fc5\u586b] \u6570\u636e\u96c6\u8bbe\u7f6e ---\n    USE_CUSTOM_DATASET = True        # [\u5fc5\u6539] True=\u81ea\u5b9a\u4e49\u6587\u4ef6\u5939, False=\u5185\u7f6e\n    CUSTOM_DATA_ROOT = \"./datasets/Intel Image Classification\" # [\u5fc5\u6539] \u6570\u636e\u96c6\u8def\u5f84\n    BUILTIN_NAME = \"CIFAR10\"         # [\u53ef\u9009] \u5185\u7f6e\u6570\u636e\u96c6\u540d\u79f0\n    DATA_DOWNLOAD_ROOT = \"./data\"    # [\u53ef\u9009] \u6570\u636e\u7f13\u5b58\u8def\u5f84\n\n    # --- [\u5fc5\u586b] \u6a21\u578b\u53c2\u6570 (\u9700\u4e0e\u8bad\u7ec3\u65f6\u4e00\u81f4) ---\n    NUM_CLASSES = 6                  # [\u5fc5\u6539] \u7c7b\u522b\u6570\u91cf (Intel=6, Cassava=5)\n    IMG_SIZE = 224                   # [\u5fc5\u6539] \u56fe\u7247\u8f93\u5165\u5c3a\u5bf8 (\u5f71\u54cd\u901f\u5ea6\u548cFLOPs\u8ba1\u7b97)\n    BATCH_SIZE = 32                  # [\u5fae\u8c03] \u6279\u6b21\u5927\u5c0f\n\n    # --- [\u53ef\u9009] \u786c\u4ef6\u4e0e\u4fdd\u5b58 ---\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    SAVE_DIR = \"./benchmark_results\" # \u7ed3\u679c\u4fdd\u5b58\u76ee\u5f55\n\n# ==========================================\n# 2. \u6838\u5fc3\u5de5\u5177\u51fd\u6570\n# ==========================================\ndef load_my_model(info):\n    \"\"\"\u52a0\u8f7d\u672c\u5730\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u6743\u91cd\"\"\"\n    print(f\"\ud83d\udd39 \u6b63\u5728\u52a0\u8f7d: {info['name']} ({info['arch']})...\")\n\n    # 1. \u521b\u5efa\u6a21\u578b\u9aa8\u67b6\n    try:\n        model = timm.create_model(info['arch'], pretrained=False, num_classes=Config.NUM_CLASSES)\n    except Exception as e:\n        print(f\"   \u274c \u67b6\u6784\u540d '{info['arch']}' \u9519\u8bef\u6216\u4e0d\u652f\u6301: {e}\")\n        return None\n\n    # 2. \u68c0\u67e5\u6587\u4ef6\u662f\u5426\u5b58\u5728\n    if not os.path.exists(info['path']):\n        print(f\"   \u274c \u627e\u4e0d\u5230\u6743\u91cd\u6587\u4ef6: {info['path']}\")\n        return None\n\n    # 3. \u52a0\u8f7d\u6743\u91cd (\u517c\u5bb9\u5904\u7406 state_dict)\n    try:\n        checkpoint = torch.load(info['path'], map_location=Config.DEVICE)\n        # \u6709\u4e9bcheckpoint\u4fdd\u5b58\u6574\u4e2adict\uff0c\u6709\u4e9b\u53ea\u4fdd\u5b58\u6743\u91cd\uff0c\u8fd9\u91cc\u505a\u81ea\u9002\u5e94\u5904\u7406\n        state_dict = checkpoint['state_dict'] if isinstance(checkpoint, dict) and 'state_dict' in checkpoint else checkpoint\n        model.load_state_dict(state_dict)\n\n        model.to(Config.DEVICE)\n        model.eval()\n        return model\n    except Exception as e:\n        print(f\"   \u274c \u6743\u91cd\u52a0\u8f7d\u5931\u8d25 (\u67b6\u6784\u4e0d\u5339\u914d?): {e}\")\n        return None\n\ndef get_model_size_mb(path):\n    \"\"\"\u83b7\u53d6\u6a21\u578b\u6587\u4ef6\u5927\u5c0f (MB)\"\"\"\n    return os.path.getsize(path) / (1024 * 1024)\n\ndef get_params_count(model):\n    \"\"\"\u8ba1\u7b97\u53c2\u6570\u91cf (Million)\"\"\"\n    return sum(p.numel() for p in model.parameters()) / 1e6\n\ndef get_flops(model):\n    \"\"\"\u8ba1\u7b97\u8ba1\u7b97\u91cf (GFLOPs)\"\"\"\n    if not HAS_THOP: return 0\n    input = torch.randn(1, 3, Config.IMG_SIZE, Config.IMG_SIZE).to(Config.DEVICE)\n    try:\n        # thop \u5e93\u7528\u4e8e\u8ba1\u7b97 FLOPs\n        flops, params = profile(model, inputs=(input, ), verbose=False)\n        return flops / 1e9\n    except:\n        return 0\n\ndef measure_speed(model, repetitions=50):\n    \"\"\"\u6d4b\u8bd5\u63a8\u7406\u901f\u5ea6 (FPS &amp; Latency)\"\"\"\n    input = torch.randn(1, 3, Config.IMG_SIZE, Config.IMG_SIZE).to(Config.DEVICE)\n\n    # \u9884\u70ed GPU (\u6d88\u9664\u9996\u6b21\u8fd0\u884c\u5f00\u9500)\n    with torch.no_grad():\n        for _ in range(10): model(input)\n\n    # \u6b63\u5f0f\u8ba1\u65f6\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    start = time.time()\n    with torch.no_grad():\n        for _ in range(repetitions): model(input)\n    torch.cuda.synchronize() if torch.cuda.is_available() else None\n    end = time.time()\n\n    avg_latency = (end - start) / repetitions * 1000 # ms\n    fps = 1000 / avg_latency\n    return avg_latency, fps\n\ndef evaluate_accuracy(model, dataloader):\n    \"\"\"\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u51c6\u786e\u7387\"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for imgs, labels in tqdm(dataloader, desc=\"   Eval\", leave=False):\n            imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)\n            outputs = model(imgs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\ndef get_dataloader():\n    \"\"\"\u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e (\u4f18\u5148\u4f7f\u7528 test \u76ee\u5f55\uff0c\u6ca1\u6709\u5219\u590d\u7528 val)\"\"\"\n    tf = transforms.Compose([\n        transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    if not Config.USE_CUSTOM_DATASET:\n        # \u5185\u7f6e\u6570\u636e\u96c6\u903b\u8f91\n        try: DatasetClass = getattr(datasets, Config.BUILTIN_NAME)\n        except: return None\n        ds = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=False, download=True, transform=tf)\n    else:\n        # \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u903b\u8f91\n        test_path = os.path.join(Config.CUSTOM_DATA_ROOT, \"test\")\n        val_path = os.path.join(Config.CUSTOM_DATA_ROOT, \"val\")\n\n        if os.path.exists(test_path):\n            print(f\"[Data] \u4f7f\u7528\u6d4b\u8bd5\u96c6: {test_path}\")\n            ds = datasets.ImageFolder(test_path, transform=tf)\n        elif os.path.exists(val_path):\n            print(f\"[Data] \u672a\u627e\u5230\u72ec\u7acb\u6d4b\u8bd5\u96c6\uff0c\u590d\u7528\u9a8c\u8bc1\u96c6: {val_path}\")\n            ds = datasets.ImageFolder(val_path, transform=tf)\n        else:\n            print(f\"\u274c \u9519\u8bef: \u5728 {Config.CUSTOM_DATA_ROOT} \u4e0b\u672a\u627e\u5230\u6570\u636e\")\n            return None\n\n    return DataLoader(ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n\ndef generate_safe_filename(models_list):\n    \"\"\"\u81ea\u52a8\u751f\u6210\u552f\u4e00\u4e14\u5408\u6cd5\u7684\u6587\u4ef6\u540d (\u57fa\u4e8e\u6a21\u578b\u540d\u79f0\u62fc\u63a5)\"\"\"\n    names = [m['name'] for m in models_list]\n    joined_name = \"_vs_\".join(names)\n    # \u66ff\u6362\u975e\u6cd5\u5b57\u7b26\u5e76\u9650\u5236\u957f\u5ea6\n    safe_name = re.sub(r'[\\\\/*?:\"&lt;&gt;| ]', '_', joined_name)\n    if len(safe_name) &gt; 100: safe_name = safe_name[:100] + \"_etc\"\n    return \"Compare_\" + safe_name\n\n# ==========================================\n# 3. \u4e3b\u7a0b\u5e8f\u5165\u53e3\n# ==========================================\nif __name__ == \"__main__\":\n    os.makedirs(Config.SAVE_DIR, exist_ok=True)\n    print(f\"\ud83d\ude80 \u5f00\u59cb\u5bf9\u6bd4\u672c\u5730\u6a21\u578b (\u5171 {len(Config.MY_MODELS)} \u4e2a)...\")\n\n    # 1. \u51c6\u5907\u6570\u636e\n    dataloader = get_dataloader()\n    if dataloader is None: exit()\n\n    results = []\n\n    # 2. \u5faa\u73af\u8bc4\u6d4b\u6bcf\u4e2a\u6a21\u578b\n    for info in Config.MY_MODELS:\n        model = load_my_model(info)\n        if model is None: continue\n\n        # \u91c7\u96c6\u6307\u6807\n        params = get_params_count(model)\n        size_mb = get_model_size_mb(info['path'])\n        flops = get_flops(model)\n        latency, fps = measure_speed(model)\n        acc = evaluate_accuracy(model, dataloader)\n\n        print(f\"   \u2705 Acc: {acc:.2f}% | FPS: {fps:.1f} | Params: {params:.2f}M\")\n\n        results.append({\n            \"Model\": info['name'],\n            \"Accuracy (%)\": acc,\n            \"Parameters (M)\": params,\n            \"FLOPs (G)\": flops,\n            \"Model Size (MB)\": size_mb,\n            \"Inference Speed (FPS)\": fps,\n        })\n\n    if not results:\n        print(\"\u274c \u672a\u4ea7\u751f\u4efb\u4f55\u6709\u6548\u7ed3\u679c\uff0c\u8bf7\u68c0\u67e5\u6a21\u578b\u8def\u5f84\u3002\")\n        exit()\n\n    # 3. \u4fdd\u5b58 CSV \u62a5\u544a\n    base_name = generate_safe_filename(Config.MY_MODELS)\n    csv_path = os.path.join(Config.SAVE_DIR, base_name + \".csv\")\n    png_path = os.path.join(Config.SAVE_DIR, base_name + \".png\")\n\n    df = pd.DataFrame(results)\n    print(\"\\n\" + \"=\"*50)\n    print(f\"\ud83c\udfc6 \u8be6\u7ec6\u62a5\u544a\u5df2\u751f\u6210: {csv_path}\")\n    print(\"=\"*50)\n    print(df.to_string(index=False))\n    df.to_csv(csv_path, index=False)\n\n    # 4. \u751f\u6210\u53ef\u89c6\u5316\u56fe\u8868\n    metrics = [\"Accuracy (%)\", \"Parameters (M)\", \"FLOPs (G)\", \"Model Size (MB)\", \"Inference Speed (FPS)\"]\n    valid_metrics = metrics if HAS_THOP else [m for m in metrics if \"FLOPs\" not in m]\n\n    plt.figure(figsize=(18, 10))\n    for i, metric in enumerate(valid_metrics):\n        rows = 2\n        cols = (len(valid_metrics) + 1) // 2\n        plt.subplot(rows, cols, i+1)\n\n        # \u7ed8\u5236\u67f1\u72b6\u56fe\n        sns.barplot(x=\"Model\", y=metric, data=df, palette=\"viridis\")\n        plt.title(metric, fontsize=14, fontweight='bold')\n        plt.xticks(rotation=45)\n\n        # \u5728\u67f1\u5b50\u4e0a\u6807\u6ce8\u5177\u4f53\u6570\u503c\n        for index, row in df.iterrows():\n             plt.text(index, row[metric], round(row[metric], 2), color='black', ha=\"center\", va=\"bottom\")\n\n    plt.tight_layout()\n    plt.savefig(png_path)\n    print(f\"\\n\ud83d\udcca \u53ef\u89c6\u5316\u56fe\u8868\u5df2\u4fdd\u5b58: {png_path}\")\n</code></pre>", "tags": ["Python", "\u6a21\u578b\u6027\u80fd"]}, {"location": "cv/Image_Classification/", "title": "\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u901a\u7528\u6a21\u677f", "text": "<p>\u6458\u8981\uff1a\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u4ee3\u7801\u6a21\u677f\uff0c\u7528\u4e8e\u8bad\u7ec3\u56fe\u50cf\u5206\u7c7b\u6a21\u578b</p>", "tags": ["Python", "\u8ba1\u7b97\u673a\u89c6\u89c9", "\u56fe\u50cf\u5206\u7c7b"]}, {"location": "cv/Image_Classification/#_2", "title": "\u6838\u5fc3\u4ee3\u7801\uff1a", "text": "<pre><code>import os\nimport sys\nimport time\nimport random\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nimport timm\nfrom timm.data import resolve_data_config\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# \u5c1d\u8bd5\u5bfc\u5165\u9ad8\u7ea7\u8bc4\u4f30\u5e93\ntry:\n    from sklearn.metrics import classification_report, confusion_matrix\n    import seaborn as sns\n    HAS_SKLEARN = True\nexcept ImportError:\n    HAS_SKLEARN = False\n    print(\"[Warning] \u672a\u5b89\u88c5 sklearn \u6216 seaborn\uff0c\u5c06\u8df3\u8fc7\u6df7\u6dc6\u77e9\u9635\u7ed8\u5236\u3002\")\n\n# ==========================================\n# 1. \u5168\u5c40\u914d\u7f6e [\u6838\u5fc3\u4fee\u6539\u533a]\n# ==========================================\nclass Config:\n    # --- \u6570\u636e\u96c6\u8def\u5f84\u8bbe\u7f6e ---\n    USE_CUSTOM_DATASET = True        # [\u5fc5\u6539] True=\u4f7f\u7528\u81ea\u5b9a\u4e49\u6587\u4ef6\u5939, False=\u4f7f\u7528\u5185\u7f6e(CIFAR10\u7b49)\n    CUSTOM_DATA_ROOT = \"flower_data\" # [\u5fc5\u6539] \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6839\u76ee\u5f55 (\u5305\u542b train/val)\n    BUILTIN_NAME = \"CIFAR10\"         # [\u53ef\u9009] \u5185\u7f6e\u6570\u636e\u96c6\u540d\u79f0 (\u4ec5\u5f53\u4e0a\u9762\u4e3aFalse\u65f6\u751f\u6548)\n    DATA_DOWNLOAD_ROOT = \"./data\"    # [\u53ef\u9009] \u6570\u636e\u4e0b\u8f7d\u7f13\u5b58\u8def\u5f84\n\n    # --- \u7ed3\u679c\u4fdd\u5b58\u8bbe\u7f6e ---\n    SAVE_DIR_ROOT = \"./results\"      # [\u53ef\u9009] \u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u6839\u76ee\u5f55\n    SAVE_DIR = \"\"                    # (\u7a0b\u5e8f\u81ea\u52a8\u751f\u6210\uff0c\u65e0\u9700\u4fee\u6539)\n\n    # --- \u6a21\u578b\u4e0e\u8bad\u7ec3\u8bbe\u7f6e ---\n    MODEL_NAME = \"resnet50\"          # [\u53ef\u9009] \u6a21\u578b\u540d\u79f0 (\u5982 resnet50, efficientnet_b0)\n    CHECKPOINT_PATH = \"\"             # [\u53ef\u9009] \u9884\u8bad\u7ec3\u6743\u91cd\u8def\u5f84 (\u7a7a\u5219\u4e0b\u8f7dImageNet\u6743\u91cd)\n    RESUME_PATH = \"\"                 # [\u53ef\u9009] \u65ad\u70b9\u7eed\u8bad\u7684 .pth \u6587\u4ef6\u8def\u5f84\n    NUM_CLASSES = 0                  # (\u7a0b\u5e8f\u81ea\u52a8\u8bc6\u522b\uff0c\u65e0\u9700\u4fee\u6539)\n\n    # --- \u8d85\u53c2\u6570\u8bbe\u7f6e ---\n    BATCH_SIZE = 32                  # [\u5fae\u8c03] \u6279\u6b21\u5927\u5c0f (\u663e\u5b58\u4e0d\u8db3\u6539\u5c0f)\n    EPOCHS = 50                      # [\u5fae\u8c03] \u8bad\u7ec3\u8f6e\u6570\n    LR = 1e-4                        # [\u5fae\u8c03] \u521d\u59cb\u5b66\u4e60\u7387 (\u5fae\u8c03\u901a\u5e38\u7528 1e-4 \u6216 1e-5)\n    WEIGHT_DECAY = 1e-4              # [\u5fae\u8c03] \u6b63\u5219\u5316\u7cfb\u6570 (\u6297\u8fc7\u62df\u5408\u7528)\n    SEED = 42                        # [\u53ef\u9009] \u968f\u673a\u79cd\u5b50 (\u4fdd\u8bc1\u7ed3\u679c\u53ef\u590d\u73b0)\n\n    # --- \u4f18\u5316\u7b56\u7565 ---\n    OPTIMIZER_NAME = 'adamw'         # [\u53ef\u9009] \u4f18\u5316\u5668: 'adamw', 'adam', 'sgd'\n    SCHEDULER_NAME = 'plateau'       # [\u53ef\u9009] \u5b66\u4e60\u7387\u7b56\u7565: 'plateau'(\u76d1\u63a7), 'cosine'(\u4f59\u5f26), 'step'\n\n    # --- \u65e9\u505c (Early Stopping) ---\n    EARLY_STOP_PATIENCE = 7          # [\u53ef\u9009] \u8fde\u7eed\u591a\u5c11\u8f6e\u4e0d\u6da8\u5206\u5c31\u505c\u6b62 (0\u8868\u793a\u5173\u95ed)\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ==========================================\n# 2. \u8f85\u52a9\u5de5\u5177 (\u65e5\u5fd7/\u968f\u673a\u79cd\u5b50/\u65e9\u505c)\n# ==========================================\ndef setup_logger(save_dir):\n    \"\"\"\u914d\u7f6e\u65e5\u5fd7\u7cfb\u7edf\uff1a\u540c\u65f6\u8f93\u51fa\u5230\u63a7\u5236\u53f0\u548c\u6587\u4ef6\"\"\"\n    log_format = '%(asctime)s - %(message)s'\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    # \u6587\u4ef6\u65e5\u5fd7 (UTF-8\u7f16\u7801)\n    file_handler = logging.FileHandler(os.path.join(save_dir, \"train.log\"), encoding='utf-8')\n    file_handler.setFormatter(logging.Formatter(log_format))\n    logger.addHandler(file_handler)\n\n    # \u63a7\u5236\u53f0\u65e5\u5fd7\n    stream_handler = logging.StreamHandler(sys.stdout)\n    stream_handler.setFormatter(logging.Formatter(log_format))\n    logger.addHandler(stream_handler)\n    return logger\n\ndef seed_everything(seed):\n    \"\"\"\u56fa\u5b9a\u6240\u6709\u968f\u673a\u79cd\u5b50\u4ee5\u4fdd\u8bc1\u5b9e\u9a8c\u53ef\u590d\u73b0\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nclass EarlyStopping:\n    \"\"\"\u65e9\u505c\u63a7\u5236\u5668\uff1a\u5f53\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u4e0d\u518d\u63d0\u5347\u65f6\u63d0\u524d\u7ec8\u6b62\u8bad\u7ec3\"\"\"\n    def __init__(self, patience=7, delta=0):\n        self.patience = patience\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n\n    def __call__(self, val_acc):\n        if not self.patience or self.patience &lt;= 0: return\n\n        # \u5982\u679c\u662f\u7b2c\u4e00\u6b21\u8bb0\u5f55\n        if self.best_score is None:\n            self.best_score = val_acc\n        # \u5982\u679c\u5f53\u524d\u5206\u6570\u6ca1\u6709\u660e\u663e\u63d0\u5347\n        elif val_acc &lt; self.best_score + self.delta:\n            self.counter += 1\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        # \u5982\u679c\u6709\u63d0\u5347\uff0c\u91cd\u7f6e\u8ba1\u6570\u5668\n        else:\n            self.best_score = val_acc\n            self.counter = 0\n\ndef get_optimizer(model):\n    \"\"\"\u6839\u636e\u914d\u7f6e\u521b\u5efa\u4f18\u5316\u5668\"\"\"\n    name = Config.OPTIMIZER_NAME.lower()\n    p = model.parameters()\n    if name == 'adamw': return optim.AdamW(p, lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n    elif name == 'sgd': return optim.SGD(p, lr=Config.LR, momentum=0.9, weight_decay=Config.WEIGHT_DECAY)\n    else: return optim.Adam(p, lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)\n\ndef get_scheduler(optimizer):\n    \"\"\"\u6839\u636e\u914d\u7f6e\u521b\u5efa\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\"\"\"\n    name = Config.SCHEDULER_NAME.lower()\n    if name == 'plateau': return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n    elif name == 'cosine': return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.EPOCHS, eta_min=1e-6)\n    elif name == 'step': return optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n    return None\n\n# ==========================================\n# 3. \u6570\u636e\u52a0\u8f7d\u4e0e\u5904\u7406\n# ==========================================\ndef get_transforms(model_cfg):\n    \"\"\"\u6839\u636e\u6a21\u578b\u9ed8\u8ba4\u53c2\u6570\u81ea\u52a8\u751f\u6210\u9884\u5904\u7406\u7ba1\u7ebf\"\"\"\n    input_size = model_cfg.get('input_size', (3, 224, 224))\n    crop_size = input_size[1]\n    mean = model_cfg.get('mean', [0.485, 0.456, 0.406])\n    std = model_cfg.get('std', [0.229, 0.224, 0.225])\n\n    # \u8bad\u7ec3\u96c6\u589e\u5f3a\uff1a\u968f\u673a\u88c1\u526a\u3001\u7ffb\u8f6c\u3001\u65cb\u8f6c\u3001\u989c\u8272\u6296\u52a8\n    train_tf = transforms.Compose([\n        transforms.Resize((crop_size, crop_size)),\n        transforms.RandomHorizontalFlip(0.5),\n        transforms.RandomRotation(15),\n        transforms.ColorJitter(0.1, 0.1, 0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    # \u9a8c\u8bc1\u96c6\u5904\u7406\uff1a\u4ec5\u8c03\u6574\u5927\u5c0f\u548c\u5f52\u4e00\u5316\n    val_tf = transforms.Compose([\n        transforms.Resize((crop_size, crop_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n    return train_tf, val_tf\n\ndef get_dataloaders(train_tf, val_tf, logger):\n    \"\"\"\n    \u667a\u80fd\u52a0\u8f7d\u6570\u636e\u96c6\n    \u4f18\u5316\u70b9\uff1a\u5982\u679c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e2d\u6ca1\u6709 test \u76ee\u5f55\uff0c\u81ea\u52a8\u590d\u7528 val \u96c6\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\n    \u4fdd\u8bc1\u540e\u7eed\u8bc4\u4f30\u4ee3\u7801\u4e0d\u62a5\u9519\u3002\n    \"\"\"\n    # --- \u5206\u652f1: \u5185\u7f6e\u6570\u636e\u96c6 (CIFAR10\u7b49) ---\n    if not Config.USE_CUSTOM_DATASET:\n        logger.info(f\"[Data] \u52a0\u8f7d\u5185\u7f6e\u6570\u636e\u96c6: {Config.BUILTIN_NAME}\")\n        try: DatasetClass = getattr(datasets, Config.BUILTIN_NAME)\n        except AttributeError: raise ValueError(f\"\u4e0d\u652f\u6301\u7684\u6570\u636e\u96c6: {Config.BUILTIN_NAME}\")\n\n        full_ds = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=True, download=True, transform=train_tf)\n        test_ds = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=False, download=True, transform=val_tf)\n\n        # \u5212\u5206 90% \u8bad\u7ec3, 10% \u9a8c\u8bc1\n        train_sz = int(0.9 * len(full_ds))\n        train_ds, val_ds = random_split(full_ds, [train_sz, len(full_ds)-train_sz])\n        class_names = full_ds.classes\n\n    # --- \u5206\u652f2: \u81ea\u5b9a\u4e49\u6587\u4ef6\u5939\u6570\u636e\u96c6 ---\n    else:\n        logger.info(f\"[Data] \u52a0\u8f7d\u81ea\u5b9a\u4e49\u6587\u4ef6\u5939: {Config.CUSTOM_DATA_ROOT}\")\n        train_dir = os.path.join(Config.CUSTOM_DATA_ROOT, \"train\")\n        val_dir = os.path.join(Config.CUSTOM_DATA_ROOT, \"val\")\n        test_dir = os.path.join(Config.CUSTOM_DATA_ROOT, \"test\")\n\n        if not os.path.exists(train_dir): raise FileNotFoundError(f\"\u7f3a\u5931\u76ee\u5f55: {train_dir}\")\n        train_ds = datasets.ImageFolder(train_dir, transform=train_tf)\n        val_ds = datasets.ImageFolder(val_dir, transform=val_tf)\n\n        # [\u903b\u8f91\u4f18\u5316] \u68c0\u67e5\u6d4b\u8bd5\u96c6\u662f\u5426\u5b58\u5728\n        if os.path.exists(test_dir):\n            logger.info(\"[Data] \u53d1\u73b0\u72ec\u7acb\u6d4b\u8bd5\u96c6 test/\")\n            test_ds = datasets.ImageFolder(test_dir, transform=val_tf)\n        else:\n            logger.info(\"[Data] \u672a\u53d1\u73b0\u72ec\u7acb\u6d4b\u8bd5\u96c6\uff0c\u5c06\u590d\u7528\u9a8c\u8bc1\u96c6(val)\u8fdb\u884c\u6700\u7ec8\u8bc4\u4f30\")\n            test_ds = val_ds \n\n        class_names = train_ds.classes\n\n    # \u66f4\u65b0\u5168\u5c40\u914d\u7f6e\n    Config.NUM_CLASSES = len(class_names)\n    logger.info(f\"[Data] \u7c7b\u522b\u6570: {Config.NUM_CLASSES}\")\n\n    # \u521b\u5efaDataLoader\n    train_dl = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=4)\n    val_dl = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4)\n    test_dl = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=4)\n\n    return train_dl, val_dl, test_dl, class_names\n\n# ==========================================\n# 4. \u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u903b\u8f91\n# ==========================================\ndef train_one_epoch(model, loader, criterion, optimizer, epoch):\n    \"\"\"\u8bad\u7ec3\u4e00\u4e2a Epoch\"\"\"\n    model.train()\n    total_loss, total_correct = 0.0, 0\n    bar = tqdm(loader, desc=f\"Epoch {epoch}/{Config.EPOCHS} [Train]\", leave=False)\n\n    for imgs, labels in bar:\n        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * imgs.size(0)\n        total_correct += (outputs.argmax(1) == labels).sum().item()\n        bar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n\n    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n\n@torch.no_grad()\ndef validate(model, loader, criterion, epoch, phase=\"Val\"):\n    \"\"\"\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\"\"\"\n    model.eval()\n    total_loss, total_correct = 0.0, 0\n    bar = tqdm(loader, desc=f\"Epoch {epoch}/{Config.EPOCHS} [{phase}]  \", leave=False)\n\n    for imgs, labels in bar:\n        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n\n        total_loss += loss.item() * imgs.size(0)\n        total_correct += (outputs.argmax(1) == labels).sum().item()\n        bar.set_postfix(loss=loss.item())\n\n    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n\ndef evaluate_test_set(model, loader, class_names, logger):\n    \"\"\"\u8bad\u7ec3\u7ed3\u675f\u540e\u8bc4\u4f30\u5e76\u5728\u65e5\u5fd7\u4e2d\u7ed8\u5236\u6df7\u6dc6\u77e9\u9635\"\"\"\n    if not loader or not HAS_SKLEARN: return\n    logger.info(\"[Test] \u6267\u884c\u6700\u7ec8\u8bc4\u4f30...\")\n    model.eval()\n    preds, targets = [], []\n\n    with torch.no_grad():\n        for imgs, labels in tqdm(loader, desc=\"Testing\"):\n            imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)\n            outputs = model(imgs)\n            preds.extend(outputs.argmax(1).cpu().numpy())\n            targets.extend(labels.cpu().numpy())\n\n    # \u6253\u5370\u5206\u7c7b\u62a5\u544a\n    report = classification_report(targets, preds, target_names=class_names, digits=4)\n    logger.info(\"\\n\" + report)\n\n    # \u7ed8\u5236\u5e76\u4fdd\u5b58\u6df7\u6dc6\u77e9\u9635\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(confusion_matrix(targets, preds), annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.savefig(os.path.join(Config.SAVE_DIR, 'confusion_matrix.png'))\n    logger.info(\"[Info] \u6df7\u6dc6\u77e9\u9635\u5df2\u4fdd\u5b58\")\n\ndef plot_history(h, save_dir, logger):\n    \"\"\"\u7ed8\u5236\u8bad\u7ec3\u66f2\u7ebf\u56fe\"\"\"\n    epochs = range(1, len(h['train_acc']) + 1)\n    plt.figure(figsize=(12, 5))\n\n    # \u51c6\u786e\u7387\u66f2\u7ebf\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, h['train_acc'], label='Train')\n    plt.plot(epochs, h['val_acc'], label='Val')\n    plt.legend(); plt.title('Accuracy')\n\n    # Loss\u66f2\u7ebf\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, h['train_loss'], label='Train')\n    plt.plot(epochs, h['val_loss'], label='Val')\n    plt.legend(); plt.title('Loss')\n\n    plt.savefig(os.path.join(save_dir, 'training_curve.png'))\n    logger.info(\"[Info] \u8bad\u7ec3\u66f2\u7ebf\u5df2\u4fdd\u5b58\")\n\ndef save_checkpoint(state, is_best, filename='last.pth'):\n    \"\"\"\u4fdd\u5b58\u6a21\u578b\u6743\u91cd\"\"\"\n    path = os.path.join(Config.SAVE_DIR, filename)\n    torch.save(state, path)\n    if is_best: torch.save(state, os.path.join(Config.SAVE_DIR, 'best_model.pth'))\n\n# ==========================================\n# 5. \u4e3b\u7a0b\u5e8f\u5165\u53e3\n# ==========================================\nif __name__ == \"__main__\":\n    seed_everything(Config.SEED)\n\n    # 1. \u521d\u59cb\u5316\u4fdd\u5b58\u76ee\u5f55 (\u683c\u5f0f: \u6a21\u578b\u540d_\u6570\u636e\u96c6\u540d_\u65f6\u95f4)\n    if Config.RESUME_PATH:\n        Config.SAVE_DIR = os.path.dirname(Config.RESUME_PATH)\n    else:\n        if Config.USE_CUSTOM_DATASET:\n            ds_name = os.path.basename(Config.CUSTOM_DATA_ROOT)\n        else:\n            ds_name = Config.BUILTIN_NAME\n        run_name = f\"{Config.MODEL_NAME}_{ds_name}_{time.strftime('%Y%m%d_%H%M%S')}\"\n        Config.SAVE_DIR = os.path.join(Config.SAVE_DIR_ROOT, run_name)\n        os.makedirs(Config.SAVE_DIR, exist_ok=True)\n\n    logger = setup_logger(Config.SAVE_DIR)\n    logger.info(f\"[Config] \u4fdd\u5b58\u76ee\u5f55: {Config.SAVE_DIR}\")\n\n    # 2. \u51c6\u5907\u6570\u636e\n    tmp_model = timm.create_model(Config.MODEL_NAME, pretrained=True)\n    cfg = resolve_data_config({}, model=tmp_model)\n    del tmp_model # \u6e05\u7406\u4e34\u65f6\u6a21\u578b\n\n    train_tf, val_tf = get_transforms(cfg)\n    train_dl, val_dl, test_dl, class_names = get_dataloaders(train_tf, val_tf, logger)\n    logger.info(f\"[Data] \u7c7b\u522b\u5217\u8868: {class_names}\")\n\n    # 3. \u521d\u59cb\u5316\u6a21\u578b\n    logger.info(f\"[Init] \u521b\u5efa\u6a21\u578b: {Config.MODEL_NAME}\")\n    if not Config.RESUME_PATH and Config.CHECKPOINT_PATH and os.path.exists(Config.CHECKPOINT_PATH):\n        logger.info(f\"[Load] \u52a0\u8f7d\u672c\u5730\u9884\u8bad\u7ec3\u6743\u91cd: {Config.CHECKPOINT_PATH}\")\n        model = timm.create_model(Config.MODEL_NAME, pretrained=False, num_classes=Config.NUM_CLASSES, checkpoint_path=Config.CHECKPOINT_PATH)\n    else:\n        model = timm.create_model(Config.MODEL_NAME, pretrained=True, num_classes=Config.NUM_CLASSES)\n\n    model.to(Config.DEVICE)\n\n    # 4. \u4f18\u5316\u5668\u4e0e\u8c03\u5ea6\u5668\n    logger.info(f\"[Init] Opt: {Config.OPTIMIZER_NAME}, Sch: {Config.SCHEDULER_NAME}\")\n    optimizer = get_optimizer(model)\n    scheduler = get_scheduler(optimizer)\n    criterion = nn.CrossEntropyLoss() # \u5982\u9700\u6807\u7b7e\u5e73\u6ed1\u53ef\u6539\u4e3a nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    # \u521d\u59cb\u5316\u65e9\u505c\n    early_stop = None\n    if Config.EARLY_STOP_PATIENCE and Config.EARLY_STOP_PATIENCE &gt; 0:\n        logger.info(f\"[Init] \u65e9\u505c\u5f00\u542f (Patience={Config.EARLY_STOP_PATIENCE})\")\n        early_stop = EarlyStopping(patience=Config.EARLY_STOP_PATIENCE)\n\n    # 5. \u8bad\u7ec3\u5faa\u73af\n    start_epoch = 1\n    best_acc = 0.0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    # \u65ad\u70b9\u6062\u590d\u903b\u8f91\n    if Config.RESUME_PATH and os.path.exists(Config.RESUME_PATH):\n        logger.info(f\"[Resume] \u6062\u590d\u65ad\u70b9: {Config.RESUME_PATH}\")\n        ckpt = torch.load(Config.RESUME_PATH, map_location=Config.DEVICE)\n        model.load_state_dict(ckpt['state_dict'])\n        optimizer.load_state_dict(ckpt['optimizer'])\n        if scheduler and 'scheduler' in ckpt: scheduler.load_state_dict(ckpt['scheduler'])\n        start_epoch = ckpt['epoch'] + 1\n        best_acc = ckpt['best_acc']\n        history = ckpt['history']\n\n    logger.info(\"[Start] \u5f00\u59cb\u8bad\u7ec3...\")\n    for epoch in range(start_epoch, Config.EPOCHS + 1):\n        t_loss, t_acc = train_one_epoch(model, train_dl, criterion, optimizer, epoch)\n        v_loss, v_acc = validate(model, val_dl, criterion, epoch)\n\n        # \u8bb0\u5f55\u5386\u53f2\n        history['train_loss'].append(t_loss); history['train_acc'].append(t_acc)\n        history['val_loss'].append(v_loss); history['val_acc'].append(v_acc)\n\n        logger.info(f\"Epoch {epoch}: Train Acc: {t_acc:.4f} | Val Acc: {v_acc:.4f} | Loss: {t_loss:.4f}\")\n\n        # \u66f4\u65b0\u5b66\u4e60\u7387\n        if scheduler:\n            if Config.SCHEDULER_NAME == 'plateau': scheduler.step(v_acc)\n            else: scheduler.step()\n\n        # \u4fdd\u5b58\u6700\u4f73\u6a21\u578b\n        is_best = v_acc &gt; best_acc\n        if is_best:\n            best_acc = v_acc\n            logger.info(f\" -&gt; \ud83c\udf1f \u65b0\u7684\u6700\u4f73\u6a21\u578b (Acc: {best_acc:.4f})\")\n\n        save_checkpoint({\n            'epoch': epoch,\n            'state_dict': model.state_dict(),\n            'best_acc': best_acc,\n            'optimizer': optimizer.state_dict(),\n            'scheduler': scheduler.state_dict() if scheduler else None,\n            'history': history\n        }, is_best)\n\n        # \u65e9\u505c\u68c0\u6d4b\n        if early_stop:\n            early_stop(v_acc)\n            if early_stop.early_stop:\n                logger.info(\"[Stop] \u89e6\u53d1\u65e9\u505c\")\n                break\n\n    # 6. \u7ed3\u675f\u8bc4\u4f30\n    plot_history(history, Config.SAVE_DIR, logger)\n    if test_dl:\n        # \u52a0\u8f7d\u6700\u4f73\u6a21\u578b\u8fdb\u884c\u6700\u7ec8\u6d4b\u8bd5\n        best_path = os.path.join(Config.SAVE_DIR, \"best_model.pth\")\n        if os.path.exists(best_path):\n            ckpt = torch.load(best_path, map_location=Config.DEVICE)\n            model.load_state_dict(ckpt['state_dict'])\n        evaluate_test_set(model, test_dl, class_names, logger)\n\n    logger.info(\"[Done] \u5b8c\u6210\")\n</code></pre>", "tags": ["Python", "\u8ba1\u7b97\u673a\u89c6\u89c9", "\u56fe\u50cf\u5206\u7c7b"]}, {"location": "cv/Predict_img/", "title": "\u5bf9\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884c\u5355\u5f20\u56fe\u7247\u9884\u6d4b", "text": "", "tags": ["Python", "\u8ba1\u7b97\u673a\u89c6\u89c9", "\u56fe\u50cf\u5206\u7c7b"]}, {"location": "cv/Predict_img/#_2", "title": "\u6838\u5fc3\u4ee3\u7801\uff1a", "text": "<pre><code>import os\nimport torch\nimport timm\nfrom timm.data import resolve_data_config\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\n\n\n# ==========================================\n# 1. \u914d\u7f6e\u533a\u57df [\u6838\u5fc3\u4fee\u6539\u533a]\n# ==========================================\nclass Config:\n    # --- \u6a21\u578b\u4e0e\u6743\u91cd ---\n    MODEL_NAME = \"resnet50\"  # &lt;--- [\u5fc5\u987b\u4fee\u6539] \u9700\u4e0e\u8bad\u7ec3\u65f6\u4e00\u81f4\n    NUM_CLASSES = 5  # &lt;--- [\u5fc5\u987b\u4fee\u6539] \u7c7b\u522b\u6570\n    WEIGHT_PATH = \"./results/xxx/best_model.pth\"  # &lt;--- [\u5fc5\u987b\u4fee\u6539] \u6743\u91cd\u8def\u5f84\n\n    # --- \u8f93\u5165\u4e0e\u8f93\u51fa ---\n    IMAGE_PATH = \"./test_image.jpg\"  # &lt;--- [\u5fc5\u987b\u4fee\u6539] \u5f85\u9884\u6d4b\u56fe\u7247\n    # \u7c7b\u522b\u540d\u79f0 (\u6309\u8bad\u7ec3\u76ee\u5f55\u7684\u5b57\u6bcd\u987a\u5e8f\u6216\u65e5\u5fd7\u6253\u5370\u7684\u987a\u5e8f)\n    CLASS_NAMES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']  # &lt;--- [\u5fc5\u987b\u4fee\u6539]\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ==========================================\n# 2. \u6838\u5fc3\u903b\u8f91\n# ==========================================\ndef get_transforms(model):\n    \"\"\"\u83b7\u53d6\u4e0e\u6a21\u578b\u5339\u914d\u7684\u9884\u5904\u7406\"\"\"\n    config = resolve_data_config({}, model=model)\n    mean = config.get('mean', [0.485, 0.456, 0.406])\n    std = config.get('std', [0.229, 0.224, 0.225])\n    input_size = config.get('input_size', (3, 224, 224))\n    crop_size = input_size[1]\n\n    print(f\"[Info] \u9884\u5904\u7406\u914d\u7f6e: Size={crop_size}\")\n    return transforms.Compose([\n        transforms.Resize((crop_size, crop_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n\ndef load_trained_model():\n    \"\"\"\u52a0\u8f7d\u67b6\u6784\u4e0e\u6743\u91cd\"\"\"\n    print(f\"[Init] \u521b\u5efa\u6a21\u578b: {Config.MODEL_NAME}\")\n    # \u521b\u5efa\u7a7a\u6a21\u578b\n    model = timm.create_model(Config.MODEL_NAME, pretrained=False, num_classes=Config.NUM_CLASSES)\n\n    if not os.path.exists(Config.WEIGHT_PATH):\n        raise FileNotFoundError(f\"\u6743\u91cd\u6587\u4ef6\u4e0d\u5b58\u5728: {Config.WEIGHT_PATH}\")\n\n    print(f\"[Load] \u52a0\u8f7d\u6743\u91cd: {Config.WEIGHT_PATH}\")\n    checkpoint = torch.load(Config.WEIGHT_PATH, map_location=Config.DEVICE)\n\n    # \u63d0\u53d6\u53c2\u6570 (\u517c\u5bb9\u53ea\u4fdd\u5b58state_dict\u6216\u5b8c\u6574checkpoint)\n    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n    model.load_state_dict(state_dict)\n\n    model.to(Config.DEVICE)\n    model.eval()\n    return model\n\n\ndef predict(model, img_path, transform):\n    \"\"\"\u63a8\u7406\u5355\u5f20\u56fe\u7247\"\"\"\n    if not os.path.exists(img_path): raise FileNotFoundError(f\"\u56fe\u7247\u4e0d\u5b58\u5728: {img_path}\")\n\n    img_raw = Image.open(img_path).convert('RGB')\n    img_tensor = transform(img_raw).unsqueeze(0).to(Config.DEVICE)\n\n    with torch.no_grad():\n        outputs = model(img_tensor)\n        probs = F.softmax(outputs, dim=1)\n\n    topk_probs, topk_ids = torch.topk(probs, k=min(3, len(Config.CLASS_NAMES)))\n    return img_raw, topk_probs.cpu().numpy()[0], topk_ids.cpu().numpy()[0]\n\n\n# ==========================================\n# 3. \u4e3b\u7a0b\u5e8f\n# ==========================================\nif __name__ == \"__main__\":\n    try:\n        model = load_trained_model()\n        tf = get_transforms(model)\n        img, probs, ids = predict(model, Config.IMAGE_PATH, tf)\n\n        print(\"\\n\" + \"=\" * 30)\n        print(\"       PREDICTION RESULT\")\n        print(\"=\" * 30)\n\n        top1_name = Config.CLASS_NAMES[ids[0]]\n        print(f\"\ud83c\udfc6 \u9884\u6d4b\u7ed3\u679c: {top1_name} ({probs[0] * 100:.2f}%)\")\n\n        print(\"\\nTop-3 \u6982\u7387\u5206\u5e03:\")\n        for i in range(len(probs)):\n            name = Config.CLASS_NAMES[ids[i]]\n            print(f\"   {i + 1}. {name:&lt;15} : {probs[i] * 100:.2f}%\")\n\n        plt.figure(figsize=(8, 6))\n        plt.imshow(img)\n        plt.title(f\"Pred: {top1_name} ({probs[0] * 100:.1f}%)\", color='green', fontsize=14)\n        plt.axis('off')\n\n        text = \"\\n\".join([f\"{Config.CLASS_NAMES[i]}: {p * 100:.1f}%\" for p, i in zip(probs, ids)])\n        plt.text(10, 20, text, bbox=dict(facecolor='white', alpha=0.8), fontsize=10)\n\n        plt.show()\n\n    except Exception as e:\n        print(f\"[Error] {e}\")\n</code></pre>", "tags": ["Python", "\u8ba1\u7b97\u673a\u89c6\u89c9", "\u56fe\u50cf\u5206\u7c7b"]}, {"location": "cv/Split_datasets/", "title": "\u6570\u636e\u96c6\u5212\u5206", "text": "<p>\u6458\u8981\uff1a\u4e00\u4e2a\u5c06\u6570\u636e\u96c6\u4ee5<code>7:2:1</code>\u7684\u6bd4\u4f8b\u5212\u5206\u7684\u4ee3\u7801</p>", "tags": ["Python", "\u6570\u636e\u96c6"]}, {"location": "cv/Split_datasets/#_2", "title": "\u6838\u5fc3\u4ee3\u7801\uff1a", "text": "<pre><code>import os\nimport shutil\nimport random\nfrom tqdm import tqdm\n\n# ==========================================\n# \u914d\u7f6e\u533a\u57df\n# ==========================================\nsource_root = \"./raw_data\"  # &lt;--- [\u8bf7\u4fee\u6539] \u4f60\u7684\u539f\u59cb\u6570\u636e\u96c6\u8def\u5f84\ntarget_root = \"./flower_data\"  # &lt;--- [\u8bf7\u4fee\u6539] \u8f93\u51fa\u8def\u5f84\nsplit_ratio = [0.7, 0.2, 0.1]  # &lt;--- [\u53ef\u4fee\u6539] \u8bad\u7ec3:\u9a8c\u8bc1:\u6d4b\u8bd5 \u6bd4\u4f8b (\u548c\u9700\u4e3a1)\n\n\ndef split_dataset():\n    # \u68c0\u67e5\u539f\u59cb\u8def\u5f84\n    if not os.path.exists(source_root):\n        print(f\"\u9519\u8bef: \u627e\u4e0d\u5230\u8def\u5f84 {source_root}\")\n        return\n\n    # \u83b7\u53d6\u6240\u6709\u7c7b\u522b\u6587\u4ef6\u5939\n    classes = [d for d in os.listdir(source_root) if os.path.isdir(os.path.join(source_root, d))]\n    print(f\"\u53d1\u73b0\u7c7b\u522b: {classes}\")\n\n    # \u521b\u5efa\u76ee\u6807\u6587\u4ef6\u5939\u7ed3\u6784\n    for split in ['train', 'val', 'test']:\n        for cls in classes:\n            os.makedirs(os.path.join(target_root, split, cls), exist_ok=True)\n\n    # \u5f00\u59cb\u5212\u5206\n    for cls in tqdm(classes, desc=\"\u6b63\u5728\u5212\u5206\u6570\u636e\u96c6\"):\n        cls_path = os.path.join(source_root, cls)\n        # \u83b7\u53d6\u8be5\u7c7b\u522b\u4e0b\u6240\u6709\u56fe\u7247\u6587\u4ef6\n        images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n\n        # \u968f\u673a\u6253\u4e71\n        random.shuffle(images)\n\n        # \u8ba1\u7b97\u5207\u5206\u70b9\n        num_total = len(images)\n        num_train = int(num_total * split_ratio[0])\n        num_val = int(num_total * split_ratio[1])\n        # \u5269\u4e0b\u7684\u90fd\u7ed9\u6d4b\u8bd5\u96c6\uff0c\u907f\u514d\u56e0\u53d6\u6574\u4e22\u5931\u6570\u636e\n\n        train_imgs = images[:num_train]\n        val_imgs = images[num_train: num_train + num_val]\n        test_imgs = images[num_train + num_val:]\n\n        # \u590d\u5236\u6587\u4ef6\u51fd\u6570\n        def copy_files(file_list, split_name):\n            for file_name in file_list:\n                src = os.path.join(cls_path, file_name)\n                dst = os.path.join(target_root, split_name, cls, file_name)\n                shutil.copy(src, dst)\n\n        # \u6267\u884c\u590d\u5236\n        copy_files(train_imgs, 'train')\n        copy_files(val_imgs, 'val')\n        copy_files(test_imgs, 'test')\n\n        print(f\"\u7c7b\u522b [{cls}]: \u603b\u6570 {num_total} -&gt; Train:{len(train_imgs)} Val:{len(val_imgs)} Test:{len(test_imgs)}\")\n\n    print(f\"\\n[Done] \u6570\u636e\u96c6\u5212\u5206\u5b8c\u6210\uff01\u4fdd\u5b58\u5728: {target_root}\")\n\n\nif __name__ == \"__main__\":\n    split_dataset()\n</code></pre>", "tags": ["Python", "\u6570\u636e\u96c6"]}, {"location": "cv/exercises/resnet18_CIFAR10/", "title": "\u57fa\u4e8e ResNet18 \u7684 CIFAR-10 \u56fe\u50cf\u5206\u7c7b", "text": "<p>\u5728\u6df1\u5ea6\u5b66\u4e60\u7684\u5165\u95e8\u4e0e\u8fdb\u9636\u8fc7\u7a0b\u4e2d\uff0cCIFAR-10 \u662f\u6700\u7ecf\u5178\u7684\u201c\u78e8\u5200\u77f3\u201d\u3002\u8fd1\u671f\u6211\u4f7f\u7528 PyTorch \u548c <code>timm</code> \u5e93\uff0c\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684 ResNet18 \u6a21\u578b\u8fdb\u884c\u4e86\u4e00\u6b21\u5b8c\u6574\u7684\u8bad\u7ec3\u5b9e\u9a8c\u3002</p> <p>\u4ee4\u4eba\u5174\u594b\u7684\u662f\uff0c\u4ec5\u4ec5\u7ecf\u8fc7 20 \u4e2a Epoch \u7684\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5c31\u8fbe\u5230\u4e86 96.16% \u7684\u51c6\u786e\u7387 \u3002\u4ee5\u4e0b\u662f\u6211\u7684\u5b9e\u9a8c\u8bb0\u5f55\u4e0e\u8be6\u7ec6\u5206\u6790\u3002</p>"}, {"location": "cv/exercises/resnet18_CIFAR10/#1", "title": "1. \u5b9e\u9a8c\u73af\u5883\u4e0e\u914d\u7f6e", "text": "<p>\u672c\u6b21\u5b9e\u9a8c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u7684\u601d\u8def\uff0c\u52a0\u8f7d\u4e86\u5728 ImageNet \u4e0a\u9884\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u8fd9\u5927\u5927\u52a0\u901f\u4e86\u6536\u655b\u8fc7\u7a0b\u3002</p> <ul> <li> <p>\u6a21\u578b\u67b6\u6784: ResNet18 (\u4f7f\u7528 <code>timm</code> \u5e93\u7684 <code>resnet18.a1_in1k</code> \u6743\u91cd) </p> </li> <li> <p>\u6570\u636e\u96c6: CIFAR-10 (\u5305\u542b airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck \u517110\u7c7b) </p> </li> <li> <p>\u4f18\u5316\u5668: Adam </p> </li> <li> <p>\u5b66\u4e60\u7387\u7b56\u7565: Cosine (\u4f59\u5f26\u9000\u706b) </p> </li> <li> <p>\u8bad\u7ec3\u8f6e\u6570: 20 Epochs </p> </li> <li> <p>\u65e9\u505c\u7b56\u7565: \u5173\u95ed </p> </li> </ul>"}, {"location": "cv/exercises/resnet18_CIFAR10/#2", "title": "2. \u8bad\u7ec3\u8fc7\u7a0b\u5206\u6790", "text": "<p>\u901a\u8fc7\u89c2\u5bdf\u8bad\u7ec3\u66f2\u7ebf \u548c\u65e5\u5fd7\u6570\u636e\uff0c\u53ef\u4ee5\u53d1\u73b0\u6a21\u578b\u5b66\u4e60\u5f97\u975e\u5e38\u5feb\uff1a</p> <ul> <li> <p>\u6781\u901f\u6536\u655b: \u5728\u7b2c 1 \u4e2a Epoch \u7ed3\u675f\u65f6\uff0c\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\uff08Val Acc\uff09\u5c31\u5df2\u7ecf\u8fbe\u5230\u4e86 91.18% \u3002\u8fd9\u5f97\u76ca\u4e8e\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002</p> </li> <li> <p>\u7a33\u6b65\u63d0\u5347: \u6b64\u540e\u51c6\u786e\u7387\u4e00\u8def\u6500\u5347\uff0c\u5728\u7b2c 15 \u4e2a Epoch \u7a81\u7834\u4e86 96% \u7684\u5927\u5173 (96.32%) \u3002</p> </li> <li> <p>\u6700\u4f73\u6a21\u578b: \u6700\u7ec8\u5728\u7b2c 19 \u4e2a Epoch \u8fbe\u5230\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6700\u9ad8\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387 96.36% \u3002</p> </li> </ul> <p>\u4ece\u635f\u5931\u66f2\u7ebf \u6765\u770b\uff0c\u8bad\u7ec3\u96c6 Loss (\u84dd\u8272) \u6301\u7eed\u4e0b\u964d\u63a5\u8fd1\u4e8e 0\uff0c\u800c\u9a8c\u8bc1\u96c6 Loss (\u6a59\u8272) \u5728\u7b2c 8-10 \u8f6e\u5de6\u53f3\u5f00\u59cb\u8d8b\u4e8e\u5e73\u7f13\uff08\u7ea6 0.02 - 0.03 \u4e4b\u95f4\uff09\u3002\u867d\u7136\u8bad\u7ec3\u96c6\u51c6\u786e\u7387\u6700\u7ec8\u8fbe\u5230\u4e86 99.48% \uff0c\u7565\u9ad8\u4e8e\u9a8c\u8bc1\u96c6\uff0c\u5b58\u5728\u8f7b\u5fae\u7684\u8fc7\u62df\u5408\uff0c\u4f46\u8003\u8651\u5230\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u4f9d\u7136\u5f88\u9ad8\uff0c\u8fd9\u79cd\u7a0b\u5ea6\u7684\u62df\u5408\u662f\u53ef\u4ee5\u63a5\u53d7\u7684\u3002</p> <p></p>"}, {"location": "cv/exercises/resnet18_CIFAR10/#3", "title": "3. \u6a21\u578b\u8bc4\u4f30\u4e0e\u8868\u73b0", "text": "<p>\u8bad\u7ec3\u7ed3\u675f\u540e\uff0c\u6211\u5bf9\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u4e86\u6700\u7ec8\u8bc4\u4f30\uff0c\u7ed3\u679c\u5982\u4e0b\uff1a</p>"}, {"location": "cv/exercises/resnet18_CIFAR10/#31", "title": "3.1 \u6574\u4f53\u6307\u6807", "text": "<ul> <li> <p>Test Accuracy: 96.16% </p> </li> <li> <p>Macro Avg F1-Score: 0.9616 </p> </li> </ul>"}, {"location": "cv/exercises/resnet18_CIFAR10/#32", "title": "3.2 \u6df7\u6dc6\u77e9\u9635\u6df1\u5ea6\u89e3\u6790", "text": "<p>\u6df7\u6dc6\u77e9\u9635 \u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u6bd4\u51c6\u786e\u7387\u66f4\u7ec6\u8282\u7684\u89c6\u89d2\u3002\u901a\u8fc7\u89c2\u5bdf\u77e9\u9635\uff0c\u6211\u4eec\u53ef\u4ee5\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u522b\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\uff1a</p> <p>\u8868\u73b0\u6700\u597d\u7684\u7c7b\u522b (Top Performers):</p> <ul> <li>Frog (\u9752\u86d9): \u51c6\u786e\u8bc6\u522b\u4e86 980 \u5f20\uff0c\u4ec5\u6709\u6781\u5c11\u6570\u9519\u8bef\u3002</li> <li>Ship (\u8239): \u51c6\u786e\u8bc6\u522b 977 \u5f20\u3002</li> <li>Automobile (\u6c7d\u8f66): \u51c6\u786e\u8bc6\u522b 973 \u5f20\u3002</li> </ul> <p>\u5bb9\u6613\u6df7\u6dc6\u7684\u7c7b\u522b (Confusion Pairs): \u6b63\u5982\u9884\u671f\u7684\u90a3\u6837\uff0c\u4e3b\u8981\u9519\u8bef\u96c6\u4e2d\u5728\u5916\u89c2\u76f8\u4f3c\u7684\u7c7b\u522b\u4e0a\uff1a</p> <ol> <li>Cat vs Dog (\u732b\u72d7\u5927\u6218):</li> <li>\u6709 43 \u5f20\u732b\u7684\u7167\u7247\u88ab\u8bef\u8ba4\u6210\u4e86\u72d7\u3002</li> <li> <p>\u6709 42 \u5f20\u72d7\u7684\u7167\u7247\u88ab\u8bef\u8ba4\u6210\u4e86\u732b\u3002 \u8fd9\u662f\u6574\u4e2a\u6a21\u578b\u6700\u5927\u7684\u5931\u5206\u70b9\uff0c\u5bfc\u81f4 Cat \u7c7b\u7684 Recall \u4ec5\u4e3a 92.30% \u3002</p> </li> <li> <p>Automobile vs Truck (\u673a\u52a8\u8f66\u6df7\u6dc6):</p> </li> <li> <p>\u6709 17 \u5f20\u6c7d\u8f66\u88ab\u8bef\u8ba4\u4e3a\u5361\u8f66\uff0c15 \u5f20\u5361\u8f66\u88ab\u8bef\u8ba4\u4e3a\u6c7d\u8f66\u3002</p> </li> <li> <p>Airplane vs Bird (\u80cc\u666f\u5e72\u6270?):</p> </li> <li>\u6709 11 \u5f20\u98de\u673a\u88ab\u8bef\u8ba4\u4e3a\u9e1f\u3002\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u4e24\u8005\u90fd\u5e38\u51fa\u73b0\u5728\u84dd\u8272\u80cc\u666f\uff08\u5929\u7a7a\uff09\u4e2d\u3002</li> </ol> <p></p>"}, {"location": "cv/exercises/resnet18_CIFAR10/#4", "title": "4. \u603b\u7ed3", "text": "<p>\u672c\u6b21\u5b9e\u9a8c\u4f7f\u7528 ResNet18 \u914d\u5408\u4f59\u5f26\u9000\u706b\u7b56\u7565\uff0c\u5728 CIFAR-10 \u4e0a\u53d6\u5f97\u4e86\u975e\u5e38\u4ee4\u4eba\u6ee1\u610f\u7684\u7ed3\u679c\u3002</p> <ul> <li>\u4f18\u70b9: \u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u5230\u4e86 SOTA \u5165\u95e8\u7ea7\u6c34\u5e73 (96%+)\u3002</li> <li>\u6539\u8fdb\u7a7a\u95f4: \u9488\u5bf9\u201c\u732b\u72d7\u6df7\u6dc6\u201d\u7684\u95ee\u9898\uff0c\u672a\u6765\u53ef\u4ee5\u5c1d\u8bd5\u5f15\u5165\u66f4\u5f3a\u7684\u6570\u636e\u589e\u5f3a\uff08\u5982 CutMix \u6216 Mixup\uff09\u6765\u63d0\u53d6\u66f4\u7ec6\u7c92\u5ea6\u7684\u7279\u5f81\uff0c\u6216\u8005\u5c1d\u8bd5\u66f4\u5927\u53c2\u6570\u91cf\u7684\u6a21\u578b\uff08\u5982 ResNet50 \u6216 EfficientNet\uff09\u3002</li> </ul>"}, {"location": "cv/exercises/resnet18_CIFAR10/#5", "title": "5.\u4ee3\u7801", "text": "<p>\u672c\u6b21\u5b9e\u9a8c\u57fa\u4e8e\u9884\u5148\u6784\u5efa\u7684\u56fe\u50cf\u5206\u7c7b\u901a\u7528\u6846\u67b6\u8fdb\u884c\uff0c\u6b64\u5904\u4ec5\u5bf9\u5dee\u5f02\u5316\u7684\u5fae\u8c03\u90e8\u5206\uff08Fine-tuning\uff09\u8fdb\u884c\u8bf4\u660e\u3002</p> <pre><code>class Config:\n    # --- \u6570\u636e\u96c6\u8bbe\u7f6e ---\n    USE_CUSTOM_DATASET = False  # \u8bbe\u4e3aFalse\uff0c\u5373\u4f7f\u7528\u5185\u7f6e\u6570\u636e\u96c6\n    CUSTOM_DATA_ROOT = \"\"  # \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8def\u5f84\n    BUILTIN_NAME = \"CIFAR10\"  # \u5185\u7f6e\u6570\u636e\u96c6\u540d\u79f0\n    DATA_DOWNLOAD_ROOT = \"./data\"  # \u4e0b\u8f7d\u7f13\u5b58\u8def\u5f84\n\n    # --- \u7ed3\u679c\u4fdd\u5b58 ---\n    SAVE_DIR_ROOT = \"./results\"  # \u7ed3\u679c\u4fdd\u5b58\u6839\u76ee\u5f55\n    SAVE_DIR = \"\"  # (\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210)\n\n    # --- \u6a21\u578b\u8bbe\u7f6e ---\n    MODEL_NAME = \"resnet18\"  # \u6a21\u578b\u540d\u79f0 (timm\u5e93\u652f\u6301\u7684\u540d\u79f0)\n    CHECKPOINT_PATH = \"\"  # \u521d\u59cb\u9884\u8bad\u7ec3\u6743\u91cd\n    RESUME_PATH = \"\"  # \u65ad\u70b9\u7eed\u8bad\u6587\u4ef6\u8def\u5f84 (.pth)\n    NUM_CLASSES = 0  # (\u8fd0\u884c\u65f6\u81ea\u52a8\u8986\u76d6)\n\n    # --- \u8bad\u7ec3\u8d85\u53c2\u6570 ---\n    BATCH_SIZE = 32  # \u6279\u6b21\u5927\u5c0f\n    EPOCHS = 20  # \u8bad\u7ec3\u603b\u8f6e\u6570\n    LR = 1e-4  # \u521d\u59cb\u5b66\u4e60\u7387\n    WEIGHT_DECAY = 1e-4  # L2\u6b63\u5219\u5316\u7cfb\u6570\n    SEED = 42  # \u968f\u673a\u79cd\u5b50\n\n    # --- \u7b56\u7565\u9009\u62e9 ---\n    OPTIMIZER_NAME = 'adam'  # \u4f7f\u7528 'adam' \u4f18\u5316\u5668\n    SCHEDULER_NAME = 'cosine'  # \u4f7f\u7528 '\u4f59\u5f26\u9000\u706b' \u5b66\u4e60\u7387\u8c03\u5ea6\u7b56\u7565\n\n    # --- \u65e9\u505c\u8bbe\u7f6e ---\n    # 0 \u6216 None \u8868\u793a\u5173\u95ed\u65e9\u505c\uff0c&gt; 0 \u8868\u793a\u5f00\u542f\u65e9\u505c\u7684\u8010\u5fc3\u8f6e\u6570\n    EARLY_STOP_PATIENCE = 0  # \u65e9\u505c\u8010\u5fc3\u8f6e\u6570 (0=\u5173\u95ed)\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"}, {"location": "cv/exercises/resnet34_CIFAR100/", "title": "\u4f7f\u7528 ResNet34 \u8bad\u7ec3 CIFAR-100", "text": "<p>\u5728\u641e\u5b9a\u4e86 CIFAR-10\uff0810\u5206\u7c7b\uff09\u4e4b\u540e\uff0c\u6211\u51b3\u5b9a\u6311\u6218\u66f4\u9ad8\u96be\u5ea6\u7684 CIFAR-100\u3002\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 100 \u4e2a\u7c7b\u522b\u3001\u6bcf\u7c7b\u4ec5\u6709 500 \u5f20\u8bad\u7ec3\u56fe\u7684\u6570\u636e\u96c6\uff0c\u5bf9\u6a21\u578b\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u90fd\u662f\u5de8\u5927\u7684\u8003\u9a8c\u3002</p> <p>\u672c\u6b21\u5b9e\u6218\u6211\u4f7f\u7528\u4e86 ResNet34 \u6a21\u578b\uff0c\u914d\u5408\u8fc1\u79fb\u5b66\u4e60\u6280\u5de7\uff0c\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86 84.76% \u7684\u51c6\u786e\u7387\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u8e29\u5751\u8bb0\u5f55\u4e0e\u7ecf\u9a8c\u603b\u7ed3\u3002</p>"}, {"location": "cv/exercises/resnet34_CIFAR100/#1", "title": "1. \u5b9e\u9a8c\u914d\u7f6e", "text": "<p>\u4e3a\u4e86\u5e94\u5bf9 CIFAR-100 \u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6311\u6218\uff0c\u6211\u9009\u62e9\u4e86\u6bd4 ResNet18 \u66f4\u6df1\u4e00\u70b9\u7684 ResNet34\uff0c\u5e76\u4f7f\u7528\u4e86\u66f4\u73b0\u4ee3\u7684\u4f18\u5316\u7b56\u7565\u3002</p> <ul> <li>\u6a21\u578b\u67b6\u6784: ResNet34 (\u9884\u8bad\u7ec3\u6743\u91cd: <code>timm/resnet34.a1_in1k</code>)</li> <li>\u6570\u636e\u96c6: CIFAR-100 (100 Classes)</li> <li>\u4f18\u5316\u5668: Adam (Weight Decay = 1e-4)</li> <li>\u8c03\u5ea6\u5668: Cosine Annealing (\u4f59\u5f26\u9000\u706b)</li> <li>\u8bad\u7ec3\u7b56\u7565: \u8fc1\u79fb\u5b66\u4e60 (Transfer Learning) + \u65e9\u505c\u673a\u5236 (Early Stopping)</li> </ul>"}, {"location": "cv/exercises/resnet34_CIFAR100/#2", "title": "2. \u8bad\u7ec3\u7ed3\u679c\u5206\u6790", "text": "<p>\u8bad\u7ec3\u8fc7\u7a0b\u51fa\u4e4e\u610f\u6599\u7684\u987a\u5229\uff0c\u6a21\u578b\u5c55\u73b0\u4e86\u6781\u5f3a\u7684\u5b66\u4e60\u80fd\u529b\u3002</p>"}, {"location": "cv/exercises/resnet34_CIFAR100/#_1", "title": "\ud83d\udcc8 \u8bad\u7ec3\u66f2\u7ebf", "text": "<p>\u4ece\u66f2\u7ebf\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230\uff1a</p> <ul> <li>\u6781\u901f\u6536\u655b\uff1a\u5f97\u76ca\u4e8e ImageNet \u7684\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u6a21\u578b\u5728\u7b2c 5 \u4e2a Epoch \u5c31\u7a81\u7834\u4e86 80% \u7684\u51c6\u786e\u7387\u3002</li> <li>\u9ad8\u7cbe\u5ea6\uff1a\u6700\u7ec8\u6d4b\u8bd5\u96c6\u51c6\u786e\u7387\u5b9a\u683c\u5728 84.76%\u3002</li> <li>\u8fc7\u62df\u5408\u73b0\u8c61\uff1a\u89c2\u5bdf\u65e5\u5fd7\u53d1\u73b0\uff0c\u8bad\u7ec3\u96c6\u51c6\u786e\u7387\u6700\u7ec8\u8fbe\u5230\u4e86 99% \u4ee5\u4e0a\uff0c\u800c\u9a8c\u8bc1\u96c6\u505c\u7559\u5728 84% \u5de6\u53f3\u3002\u867d\u7136\u5b58\u5728\u8fc7\u62df\u5408\uff0c\u4f46\u5728\u5982\u6b64\u5c11\u7684\u6570\u636e\u91cf\uff08\u6bcf\u7c7b500\u5f20\uff09\u4e0b\uff0c\u80fd\u8fbe\u5230\u8fd9\u4e2a\u6cdb\u5316\u6027\u80fd\u5df2\u7ecf\u975e\u5e38\u4ee4\u4eba\u6ee1\u610f\u3002</li> </ul>"}, {"location": "cv/exercises/resnet34_CIFAR100/#_2", "title": "\ud83e\udde9 \u6df7\u6dc6\u77e9\u9635", "text": "<p>CIFAR-100 \u7684\u96be\u70b9\u5728\u4e8e\u6709\u5f88\u591a\u201c\u5b50\u7c7b\u201d\uff0c\u6bd4\u5982 <code>Girl</code>, <code>Boy</code>, <code>Man</code>, <code>Woman</code>\u3002 \u901a\u8fc7\u5206\u6790\u6df7\u6dc6\u77e9\u9635\uff0c\u6211\u53d1\u73b0\u6a21\u578b\u4e3b\u8981\u7684\u9519\u8bef\u90fd\u96c6\u4e2d\u5728\u8bed\u4e49\u76f8\u4f3c\u7684\u7c7b\u522b\u4e0a\uff1a</p> <ul> <li>Boy vs Girl: \u8fd9\u662f\u6700\u5bb9\u6613\u6df7\u6dc6\u7684\u4e00\u5bf9\u3002</li> <li>Maple Tree vs Oak Tree: \u690d\u7269\u7c7b\u7684\u7ec6\u5fae\u7eb9\u7406\u5dee\u5f02\u5f88\u96be\u533a\u5206\u3002</li> <li>Worm vs Caterpillar: \u5f62\u72b6\u6781\u5176\u76f8\u4f3c\u3002</li> </ul> <p>\u4f46\u5bf9\u4e8e\u7279\u5f81\u9c9c\u660e\u7684\u7c7b\u522b\uff08\u5982 <code>Orange</code> \u6a59\u5b50, <code>Motorcycle</code> \u6469\u6258\u8f66\uff09\uff0c\u6a21\u578b\u7684\u8bc6\u522b\u7387\u51e0\u4e4e\u8fbe\u5230\u4e86 100%\u3002</p>"}, {"location": "cv/exercises/resnet34_CIFAR100/#3", "title": "3. \u8e29\u5751\u4e0e\u95ee\u9898\u8bb0\u5f55", "text": "<p>\u5728\u4ece CIFAR-10 \u8fc1\u79fb\u5230 CIFAR-100 \u7684\u8fc7\u7a0b\u4e2d\uff0c\u6211\u9047\u5230\u4e86\u4e00\u4e9b\u5177\u6709\u4ee3\u8868\u6027\u7684\u95ee\u9898\uff0c\u975e\u5e38\u6709\u8bb0\u5f55\u4ef7\u503c\u3002</p>"}, {"location": "cv/exercises/resnet34_CIFAR100/#_3", "title": "\ud83d\udc1b \u95ee\u9898\u4e00\uff1a\u7ef4\u5ea6\u4e0d\u5339\u914d", "text": "<p>\u5728\u5c1d\u8bd5\u52a0\u8f7d\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u6216\u7ee7\u7eed\u8bad\u7ec3\u65f6\uff0c\u6211\u9047\u5230\u4e86\u5982\u4e0b\u62a5\u9519\uff1a</p> <p><code>RuntimeError: Error(s) in loading state_dict for ResNet: size mismatch for fc.weight: copying a param with shape torch.Size([100, 512]) from checkpoint, the shape in current model is torch.Size([1000, 512]).</code></p> <p>\u539f\u56e0\u5206\u6790\uff1a <code>timm.create_model</code> \u9ed8\u8ba4\u521b\u5efa\u7684\u662f ImageNet \u6807\u51c6\u7684 1000 \u7c7b\u6a21\u578b\uff0c\u800c\u6211\u4fdd\u5b58\u7684\u6743\u91cd\u662f\u9488\u5bf9 CIFAR-100 \u8bad\u7ec3\u7684 100 \u7c7b\u6743\u91cd\u3002\u4e5f\u5c31\u662f\u201c\u5751\u67091000\u4e2a\uff0c\u841d\u535c\u53ea\u6709100\u4e2a\u201d\uff0c\u8fd9\u5c31\u586b\u4e0d\u8fdb\u53bb\u4e86\u3002</p> <p>\u89e3\u51b3\u65b9\u6848\uff1a \u5728\u521b\u5efa\u6a21\u578b\u65f6\uff0c\u5fc5\u987b\u663e\u5f0f\u6307\u5b9a <code>num_classes</code>\uff0c\u544a\u8bc9\u6a21\u578b\u201c\u6211\u53ea\u8981100\u4e2a\u5751\u201d\uff1a</p> <pre><code># \u4fee\u6539\u524d (\u62a5\u9519)\nmodel = timm.create_model('resnet34', checkpoint_path='best.pth')\n\n# \u4fee\u6539\u540e (\u6210\u529f)\nmodel = timm.create_model('resnet34', num_classes=100, checkpoint_path='best.pth')\n</code></pre>"}, {"location": "cv/exercises/resnet34_CIFAR100/#_4", "title": "\ud83e\uddd0 \u95ee\u9898\u4e8c\uff1a\u5173\u4e8e\u51c6\u786e\u7387\u63d0\u5347\u7684\u601d\u8003", "text": "<p>\u5728\u8bad\u7ec3\u8fbe\u5230 84% \u540e\uff0c\u6211\u5c1d\u8bd5\u4e86\u66f4\u5f3a\u7684\u6570\u636e\u589e\u5f3a\uff08\u5982 <code>RandomResizedCrop</code> \u548c <code>RandomErasing</code>\uff09\u8bd5\u56fe\u51b2\u523a\u66f4\u9ad8\u5206\u6570\uff0c\u4f46\u53d1\u73b0\u6548\u679c\u5e76\u4e0d\u660e\u663e\uff0c\u751a\u81f3\u5728\u77ed\u65f6\u95f4\u5185\u5bfc\u81f4\u4e86\u51c6\u786e\u7387\u4e0b\u964d\u3002</p> <p>\u5b66\u4e60\u5fc3\u5f97\uff1a</p> <ol> <li>\u5fae\u8c03\u7684\u9677\u9631\uff1a\u5f53\u4f60\u52a0\u8f7d\u4e00\u4e2a\u5df2\u7ecf\u8bad\u7ec3\u5f97\u5f88\u597d\u7684\u6a21\u578b\uff08LR\u5df2\u964d\u5f97\u5f88\u4f4e\uff09\u8fdb\u884c\u7b2c\u4e8c\u9636\u6bb5\u5fae\u8c03\u65f6\uff0c\u5343\u4e07\u4e0d\u80fd\u4f7f\u7528\u5927\u7684\u521d\u59cb\u5b66\u4e60\u7387\uff08\u5982 1e-4\uff09\u3002\u8fd9\u4f1a\u7834\u574f\u6a21\u578b\u5df2\u6709\u7684\u7279\u5f81\u5206\u5e03\u3002\u6b63\u786e\u7684\u505a\u6cd5\u662f\u5c06\u5b66\u4e60\u7387\u964d\u4f4e 10-100 \u500d\uff08\u5982 1e-6\uff09\u3002</li> <li>\u589e\u5f3a\u7684\u4ee3\u4ef7\uff1a\u5f3a\u6570\u636e\u589e\u5f3a\u867d\u7136\u80fd\u6297\u8fc7\u62df\u5408\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u5b66\u4e60\u96be\u5ea6\uff0c\u9700\u8981\u66f4\u957f\u7684 Epochs \u6765\u6536\u655b\u3002\u5bf9\u4e8e\u5feb\u901f\u9a8c\u8bc1\u5b9e\u9a8c\uff0c\u6807\u51c6\u7684\u589e\u5f3a\uff08Flip/Crop\uff09\u5f80\u5f80\u6027\u4ef7\u6bd4\u6700\u9ad8\u3002</li> </ol>"}, {"location": "cv/exercises/resnet34_CIFAR100/#4", "title": "4. \u603b\u7ed3", "text": "<p>\u8fd9\u6b21\u5b9e\u9a8c\u8bc1\u660e\u4e86 ResNet34 \u5728\u4e2d\u7b49\u89c4\u6a21\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5f3a\u5927\u7edf\u6cbb\u529b\u3002\u4f7f\u7528 84.76% \u7684\u51c6\u786e\u7387\uff0c\u8fd9\u4e2a\u6a21\u578b\u5df2\u7ecf\u53ef\u4ee5\u5f88\u597d\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002</p>"}, {"location": "cv/exercises/resnet34_CIFAR100/#5", "title": "5.\u4ee3\u7801", "text": "<p>\u672c\u6b21\u5b9e\u9a8c\u57fa\u4e8e\u9884\u5148\u6784\u5efa\u7684\u56fe\u50cf\u5206\u7c7b\u901a\u7528\u6846\u67b6\u8fdb\u884c\uff0c\u6b64\u5904\u4ec5\u5bf9\u5dee\u5f02\u5316\u7684\u5fae\u8c03\u90e8\u5206\uff08Fine-tuning\uff09\u8fdb\u884c\u8bf4\u660e\u3002</p> <pre><code>class Config:\n    # --- \u6570\u636e\u96c6\u8bbe\u7f6e ---\n    USE_CUSTOM_DATASET = False  # \u8bbe\u4e3aFalse\uff0c\u5373\u4f7f\u7528\u5185\u7f6e\u6570\u636e\u96c6\n    CUSTOM_DATA_ROOT = \"flower_data\"  # \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8def\u5f84\n    BUILTIN_NAME = \"CIFAR100\"  # \u5185\u7f6e\u6570\u636e\u96c6\u540d\u79f0\n    DATA_DOWNLOAD_ROOT = \"./data\"  # \u4e0b\u8f7d\u7f13\u5b58\u8def\u5f84\n\n    # --- \u7ed3\u679c\u4fdd\u5b58 ---\n    SAVE_DIR_ROOT = \"./results\"  # \u7ed3\u679c\u4fdd\u5b58\u6839\u76ee\u5f55\n    SAVE_DIR = \"\"  # (\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210)\n\n    # --- \u6a21\u578b\u8bbe\u7f6e ---\n    MODEL_NAME = \"resnet34\"  # \u6a21\u578b\u540d\u79f0 (timm\u5e93\u652f\u6301\u7684\u540d\u79f0)\n    CHECKPOINT_PATH = \"\"  # \u521d\u59cb\u9884\u8bad\u7ec3\u6743\u91cd (\u8fc1\u79fb\u5b66\u4e60\u7528)\n    RESUME_PATH = \"\"  # \u65ad\u70b9\u7eed\u8bad\u6587\u4ef6\u8def\u5f84 (.pth)\n    NUM_CLASSES = 0  # (\u8fd0\u884c\u65f6\u81ea\u52a8\u8986\u76d6)\n\n    # --- \u8bad\u7ec3\u8d85\u53c2\u6570 ---\n    BATCH_SIZE = 32  # \u6279\u6b21\u5927\u5c0f\n    EPOCHS = 20  # \u8bad\u7ec3\u603b\u8f6e\u6570\n    LR = 1e-4  # \u521d\u59cb\u5b66\u4e60\u7387\n    WEIGHT_DECAY = 1e-4  # L2\u6b63\u5219\u5316\u7cfb\u6570\n    SEED = 42  # \u968f\u673a\u79cd\u5b50\n\n    # --- \u7b56\u7565\u9009\u62e9 ---\n    OPTIMIZER_NAME = 'adam'  # \u4f7f\u7528 'adam' \u4f18\u5316\u5668\n    SCHEDULER_NAME = 'cosine'  # \u4f7f\u7528 '\u4f59\u5f26\u9000\u706b' \u5b66\u4e60\u7387\u8c03\u5ea6\u7b56\u7565\n\n    # --- \u65e9\u505c\u8bbe\u7f6e ---\n    # 0 \u6216 None \u8868\u793a\u5173\u95ed\u65e9\u505c\uff0c&gt; 0 \u8868\u793a\u5f00\u542f\u65e9\u505c\u7684\u8010\u5fc3\u8f6e\u6570\n    EARLY_STOP_PATIENCE = 0  # \u65e9\u505c\u8010\u5fc3\u8f6e\u6570 (0=\u5173\u95ed)\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/", "title": "ResNet50 \u6728\u85af\u53f6\u75c5\u5bb3\u5206\u7c7b", "text": "<p>\u5728\u5b8c\u6210\u4e86\u57fa\u7840\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u540e\uff0c\u6211\u5c06\u76ee\u5149\u6295\u5411\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684 Kaggle \u7ade\u8d5b\u6570\u636e\u96c6\u2014\u2014Cassava Leaf Disease Classification\uff08\u6728\u85af\u53f6\u75c5\u5bb3\u5206\u7c7b\uff09\u3002\u8fd9\u662f\u4e00\u4e2a\u5178\u578b\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\uff0c\u4e14\u4ee5\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u8457\u79f0\u3002</p> <p>\u672c\u6b21\u5b9e\u9a8c\u6211\u4f7f\u7528\u4e86\u7ecf\u5178\u7684 ResNet50 \u6a21\u578b\uff0c\u6700\u7ec8\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u4e86 85.19% \u7684\u51c6\u786e\u7387\u3002\u867d\u7136\u6210\u7ee9\u5c1a\u53ef\uff0c\u4f46\u901a\u8fc7\u5206\u6790\u8bad\u7ec3\u65e5\u5fd7\u548c\u6df7\u6dc6\u77e9\u9635\uff0c\u6211\u53d1\u73b0\u4e86\u8bb8\u591a\u503c\u5f97\u6df1\u6316\u7684\u201c\u9690\u60a3\u201d\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u8be6\u7ec6\u590d\u76d8\u3002</p>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#1", "title": "1. \u5b9e\u9a8c\u914d\u7f6e", "text": "<ul> <li>\u6a21\u578b\u67b6\u6784: ResNet50 (\u9884\u8bad\u7ec3\u6743\u91cd: <code>timm/resnet50.a1_in1k</code>)</li> <li>\u6570\u636e\u96c6: 5 \u5206\u7c7b (CBB, CBSD, CGM, CMD, Healthy)</li> <li>\u4f18\u5316\u5668: AdamW</li> <li>\u8c03\u5ea6\u5668: Cosine Annealing (\u4f59\u5f26\u9000\u706b)</li> <li>\u65e9\u505c\u7b56\u7565: Patience = 7 (\u8010\u5fc3\u503c 7 \u8f6e)</li> </ul>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#2", "title": "2. \u8bad\u7ec3\u8fc7\u7a0b\uff1a\u8d77\u6b65\u5373\u5dc5\u5cf0\uff0c\u968f\u540e\u8fc7\u62df\u5408", "text": "<p>\u8bad\u7ec3\u8fc7\u7a0b\u975e\u5e38\u8fc5\u901f\uff0cResNet50 \u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u4f46\u4e5f\u5f88\u5feb\u89e6\u78b0\u5230\u4e86\u74f6\u9888\u3002</p> <ul> <li> <p>\u6781\u901f\u6536\u655b\uff1a \u4ec5\u4ec5\u5728 Epoch 1\uff0c\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u5c31\u8fbe\u5230\u4e86 75.65% \u3002\u8fd9\u5f97\u76ca\u4e8e ImageNet \u9884\u8bad\u7ec3\u6743\u91cd\u5bf9\u81ea\u7136\u7eb9\u7406\u7684\u826f\u597d\u9002\u5e94\u6027\u3002</p> </li> <li> <p>\u9ec4\u91d1\u65f6\u523b (Epoch 7)\uff1a \u6a21\u578b\u5728 Epoch 7 \u8fbe\u5230\u4e86\u5168\u573a\u6700\u4f73\u6027\u80fd\uff0c\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387 85.19%\uff0cLoss \u964d\u81f3\u6700\u4f4e\u70b9 0.3787 \u3002</p> </li> <li> <p>\u8fc7\u62df\u5408\u7684\u4fe1\u53f7\uff1a \u4ece\u66f2\u7ebf\u56fe\u4e2d\u53ef\u4ee5\u660e\u663e\u770b\u5230\uff0c\u5728 Epoch 7 \u4e4b\u540e\uff0c\u84dd\u8272\u7ebf\uff08\u8bad\u7ec3\u96c6\u51c6\u786e\u7387\uff09 \u4e00\u8def\u9ad8\u6b4c\u731b\u8fdb\uff0c\u6700\u7ec8\u5728 Epoch 14 \u8fbe\u5230\u4e86 92.29% \uff1b\u7136\u800c\uff0c\u6a59\u8272\u7ebf\uff08\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\uff09 \u5374\u5f00\u59cb\u505c\u6ede\u751a\u81f3\u4e0b\u6ed1\uff0c\u6700\u7ec8\u505c\u7559\u5728 83.60% \u3002 \u540c\u65f6\uff0c\u9a8c\u8bc1\u96c6\u7684 Loss \u5728 Epoch 10 \u4e4b\u540e\u5f00\u59cb\u53cd\u5f39\uff08\u4ece 0.30 \u6da8\u5230 0.54\uff09\uff0c\u8fd9\u662f\u5178\u578b\u7684\u8fc7\u62df\u5408 (Overfitting) \u5f81\u5146\u3002\u6700\u7ec8\uff0c\u65e9\u505c\u673a\u5236\u5728 Epoch 14 \u89e6\u53d1\uff0c\u7ec8\u6b62\u4e86\u8bad\u7ec3 \u3002</p> </li> </ul> <p></p>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#3", "title": "3. \u7ed3\u679c\u6df1\u5ea6\u5256\u6790\uff1a\u88ab\u201c\u5927\u7c7b\u201d\u7edf\u6cbb\u7684\u51c6\u786e\u7387", "text": "<p>\u6700\u7ec8\u7684\u8bc4\u4f30\u7ed3\u679c\u5b9a\u683c\u5728 85.19% \u3002\u4e4d\u4e00\u770b\u8fd9\u4e2a\u5206\u6570\u8fd8\u4e0d\u9519\uff0c\u4f46\u5982\u679c\u6211\u4eec\u62c6\u89e3\u5f00\u6765\u770b\uff0c\u5c31\u4f1a\u53d1\u73b0\u4e25\u91cd\u7684\u95ee\u9898\u3002</p>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#31", "title": "3.1 \u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861", "text": "<p>\u67e5\u770b\u5206\u7c7b\u62a5\u544a (Classification Report)\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6837\u672c\u5206\u5e03\u6781\u5176\u4e0d\u5747\uff1a</p> <ul> <li> <p>3_CMD (\u82b1\u53f6\u75c5): \u6837\u672c\u6570\u9ad8\u8fbe 1316 \u5f20 \u3002</p> </li> <li> <p>0_CBB (\u7ec6\u83cc\u6027\u67af\u840e\u75c5): \u6837\u672c\u6570\u4ec5\u6709 109 \u5f20 \u3002</p> </li> <li> <p>\u5dee\u8ddd: \u7c7b\u522b 3 \u7684\u6570\u636e\u91cf\u662f\u7c7b\u522b 0 \u7684 12 \u500d\uff01</p> </li> </ul>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#32", "title": "3.2 \u6df7\u6dc6\u77e9\u9635\u8bf4\u4e86\u4ec0\u4e48\uff1f", "text": "<p>\u6df7\u6dc6\u77e9\u9635\u76f4\u89c2\u5730\u5c55\u793a\u4e86\u8fd9\u79cd\u4e0d\u5e73\u8861\u5e26\u6765\u7684\u540e\u679c\uff1a</p> <ol> <li> <p>\u201c\u5b66\u9738\u201d\u7c7b\u522b (3_CMD)\uff1a \u7531\u4e8e\u6570\u636e\u91cf\u5de8\u5927\uff0c\u6a21\u578b\u5728\u8fd9\u4e2a\u7c7b\u522b\u4e0a\u8868\u73b0\u6781\u597d\uff0cRecall \u9ad8\u8fbe 94.83% \u3002\u6df7\u6dc6\u77e9\u9635\u4e2d\u53f3\u4e0b\u89d2\u90a3\u4e2a\u6df1\u84dd\u8272\u7684\u65b9\u5757 (1248 \u5f20\u6b63\u786e\u9884\u6d4b) \u5c31\u662f\u8bc1\u660e\u3002</p> </li> <li> <p>\u201c\u5b66\u6e23\u201d\u7c7b\u522b (0_CBB)\uff1a \u7531\u4e8e\u6837\u672c\u592a\u5c11\uff0c\u6a21\u578b\u6839\u672c\u5b66\u4e0d\u4f1a\u3002109 \u5f20\u56fe\u91cc\uff0c\u53ea\u6709 67 \u5f20 \u9884\u6d4b\u6b63\u786e\uff0cRecall \u4ec5\u4e3a 61.47% \u3002\u5927\u91cf\u7684 CBB \u56fe\u7247\u88ab\u8bef\u5224\u4e3a\u4e86 Healthy (24\u5f20) \u6216 CMD (8\u5f20)\u3002</p> </li> <li> <p>\u201c\u548c\u7a00\u6ce5\u201d\u73b0\u8c61\uff1a \u89c2\u5bdf\u6df7\u6dc6\u77e9\u9635\u7684\u7b2c\u56db\u5217\uff083_CMD\uff09\uff0c\u4f60\u4f1a\u53d1\u73b0\u5176\u4ed6\u6240\u6709\u7c7b\u522b\u90fd\u6709\u5927\u91cf\u56fe\u7247\u88ab\u8bef\u5224\u4e3a 3_CMD\u3002</p> </li> <li>2_CGM: \u6709 40 \u5f20\u88ab\u8bef\u5224\u4e3a CMD\u3002</li> <li>4_Healthy: \u6709 27 \u5f20\u88ab\u8bef\u5224\u4e3a CMD\u3002</li> <li>\u539f\u56e0\uff1a\u6a21\u578b\u4e3a\u4e86\u5237\u9ad8\u603b\u51c6\u786e\u7387\uff0c\u503e\u5411\u4e8e\u5c06\u6a21\u68f1\u4e24\u53ef\u7684\u56fe\u7247\u76f4\u63a5\u731c\u6210\u6570\u91cf\u6700\u591a\u7684\u7c7b\u522b\uff08CMD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u6295\u673a\u53d6\u5de7\u7684\u7b56\u7565\u3002</li> </ol> <p></p>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#4", "title": "4. \u603b\u7ed3\u4e0e\u6539\u8fdb\u65b9\u5411", "text": "<p>\u8fd9\u6b21\u5b9e\u9a8c\u8bc1\u660e\u4e86 ResNet50 \u5728\u6570\u636e\u91cf\u5145\u8db3\u7684\u60c5\u51b5\u4e0b\uff08\u5982\u7c7b\u522b 3\uff09\u80fd\u8fbe\u5230\u6781\u9ad8\u7684\u7cbe\u5ea6\uff08F1-score 0.94\uff09\uff0c\u4f46\u5728\u5c0f\u6837\u672c\u7c7b\u522b\u4e0a\u8868\u73b0\u60e8\u6de1\uff08F1-score 0.62\uff09\u300285.19% \u7684\u603b\u51c6\u786e\u7387\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u662f\u9760\u201c\u731c\u5927\u7c7b\u201d\u6491\u8d77\u6765\u7684\u865a\u9ad8\u5206\u6570\u3002</p> <p>\u63a5\u4e0b\u6765\u7684\u4f18\u5316\u6e05\u5355\uff1a</p> <ol> <li>\u5f15\u5165 Class Weights (\u7c7b\u522b\u6743\u91cd)\uff1a\u5728\u8ba1\u7b97 Loss \u65f6\uff0c\u7ed9\u6570\u91cf\u5c11\u7684\u7c7b\u522b\uff08\u5982 0_CBB\uff09\u66f4\u9ad8\u7684\u60e9\u7f5a\u6743\u91cd\uff0c\u5f3a\u8feb\u6a21\u578b\u53bb\u5173\u6ce8\u5c0f\u6837\u672c\u3002</li> <li>\u4f7f\u7528 Mixup \u6216 CutMix\uff1a\u8fd9\u662f\u89e3\u51b3 Cassava \u6570\u636e\u96c6\u8fc7\u62df\u5408\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u795e\u5668\u3002\u901a\u8fc7\u6df7\u5408\u56fe\u7247\u548c\u6807\u7b7e\uff0c\u8ba9\u6a21\u578b\u4e0d\u518d\u201c\u6b7b\u8bb0\u786c\u80cc\u201d\u3002</li> <li>\u5c1d\u8bd5 Focal Loss\uff1a\u964d\u4f4e\u6613\u5206\u7c7b\u6837\u672c\u7684\u6743\u91cd\uff0c\u4e13\u6ce8\u4e8e\u90a3\u4e9b\u96be\u5206\u7c7b\u7684\u6837\u672c\u3002</li> </ol> <p>\u8fd9\u6b21\u8bad\u7ec3\u867d\u7136\u6b62\u6b65\u4e8e 85%\uff0c\u4f46\u5b83\u6e05\u6670\u5730\u66b4\u9732\u4e86\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u75db\u70b9\uff0c\u4e3a\u4e0b\u4e00\u6b65\u7684\u8fdb\u9636\u4f18\u5316\u6307\u660e\u4e86\u65b9\u5411\uff01</p>"}, {"location": "cv/exercises/resnet50_Cassava_Leaf_Disease_Classification/#5", "title": "5. \u4ee3\u7801", "text": "<p>\u672c\u6b21\u5b9e\u9a8c\u57fa\u4e8e\u9884\u5148\u6784\u5efa\u7684\u56fe\u50cf\u5206\u7c7b\u901a\u7528\u6846\u67b6\u8fdb\u884c\uff0c\u6b64\u5904\u4ec5\u5bf9\u5dee\u5f02\u5316\u7684\u5fae\u8c03\u90e8\u5206\uff08Fine-tuning\uff09\u8fdb\u884c\u8bf4\u660e\u3002</p> <pre><code>class Config:\n    # --- \u6570\u636e\u96c6\u8def\u5f84\u8bbe\u7f6e ---\n    USE_CUSTOM_DATASET = True  # [\u5fc5\u6539] True=\u4f7f\u7528\u81ea\u5b9a\u4e49\u6587\u4ef6\u5939, False=\u4f7f\u7528\u5185\u7f6e(CIFAR10\u7b49)\n    CUSTOM_DATA_ROOT = \"./datasets/Cassava_Leaf_Disease_Classification\"  # [\u5fc5\u6539] \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u6839\u76ee\u5f55 (\u5305\u542b train/val)\n    BUILTIN_NAME = \"\"  # [\u53ef\u9009] \u5185\u7f6e\u6570\u636e\u96c6\u540d\u79f0 (\u4ec5\u5f53\u4e0a\u9762\u4e3aFalse\u65f6\u751f\u6548)\n    DATA_DOWNLOAD_ROOT = \"./data\"  # [\u53ef\u9009] \u6570\u636e\u4e0b\u8f7d\u7f13\u5b58\u8def\u5f84\n\n    # --- \u7ed3\u679c\u4fdd\u5b58\u8bbe\u7f6e ---\n    SAVE_DIR_ROOT = \"./results\"  # [\u53ef\u9009] \u8bad\u7ec3\u7ed3\u679c\u4fdd\u5b58\u6839\u76ee\u5f55\n    SAVE_DIR = \"\"  # (\u7a0b\u5e8f\u81ea\u52a8\u751f\u6210\uff0c\u65e0\u9700\u4fee\u6539)\n\n    # --- \u6a21\u578b\u4e0e\u8bad\u7ec3\u8bbe\u7f6e ---\n    MODEL_NAME = \"resnet50\"  # [\u53ef\u9009] \u6a21\u578b\u540d\u79f0 (\u5982 resnet50, efficientnet_b0)\n    CHECKPOINT_PATH = \"\"  # [\u53ef\u9009] \u9884\u8bad\u7ec3\u6743\u91cd\u8def\u5f84 (\u7a7a\u5219\u4e0b\u8f7dImageNet\u6743\u91cd)\n    RESUME_PATH = \"\"  # [\u53ef\u9009] \u65ad\u70b9\u7eed\u8bad\u7684 .pth \u6587\u4ef6\u8def\u5f84\n    NUM_CLASSES = 0  # (\u7a0b\u5e8f\u81ea\u52a8\u8bc6\u522b\uff0c\u65e0\u9700\u4fee\u6539)\n\n    # --- \u8d85\u53c2\u6570\u8bbe\u7f6e ---\n    BATCH_SIZE = 16  # [\u5fae\u8c03] \u6279\u6b21\u5927\u5c0f (\u663e\u5b58\u4e0d\u8db3\u6539\u5c0f)\n    EPOCHS = 50  # [\u5fae\u8c03] \u8bad\u7ec3\u8f6e\u6570\n    LR = 1e-4  # [\u5fae\u8c03] \u521d\u59cb\u5b66\u4e60\u7387 (\u5fae\u8c03\u901a\u5e38\u7528 1e-4 \u6216 1e-5)\n    WEIGHT_DECAY = 1e-4  # [\u5fae\u8c03] \u6b63\u5219\u5316\u7cfb\u6570 (\u6297\u8fc7\u62df\u5408\u7528)\n    SEED = 42  # [\u53ef\u9009] \u968f\u673a\u79cd\u5b50 (\u4fdd\u8bc1\u7ed3\u679c\u53ef\u590d\u73b0)\n\n    # --- \u4f18\u5316\u7b56\u7565 ---\n    OPTIMIZER_NAME = 'adamw'  # [\u53ef\u9009] \u4f18\u5316\u5668: 'adamw', 'adam', 'sgd'\n    SCHEDULER_NAME = 'cosine'  # [\u53ef\u9009] \u5b66\u4e60\u7387\u7b56\u7565: 'plateau'(\u76d1\u63a7), 'cosine'(\u4f59\u5f26), 'step'\n\n    # --- \u65e9\u505c (Early Stopping) ---\n    EARLY_STOP_PATIENCE = 7  # [\u53ef\u9009] \u8fde\u7eed\u591a\u5c11\u8f6e\u4e0d\u6da8\u5206\u5c31\u505c\u6b62 (0\u8868\u793a\u5173\u95ed)\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/", "title": "ResNet50 \u8bad\u7ec3 Intel \u56fe\u50cf\u5206\u7c7b", "text": "<p>\u5728\u5b8c\u6210\u57fa\u7840\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u540e\uff0c\u6211\u5c06\u76ee\u5149\u6295\u5411\u4e86\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684 Intel Image Classification \u6570\u636e\u96c6\u3002\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 6 \u7c7b\u81ea\u7136\u98ce\u5149\u4e0e\u57ce\u5e02\u5efa\u7b51\u7684\u6570\u636e\u96c6\uff08Buildings, Forest, Glacier, Mountain, Sea, Street\uff09\u3002</p> <p>\u672c\u6b21\u5b9e\u9a8c\u6211\u4f7f\u7528\u4e86\u5de5\u4e1a\u754c\u6700\u5e38\u7528\u7684 ResNet50 \u6a21\u578b\uff0c\u914d\u5408\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u4ec5\u4ec5\u8bad\u7ec3\u4e86 4 \u4e2a Epoch \u5c31\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u8be6\u7ec6\u590d\u76d8\u3002</p>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#1-setup", "title": "1. \u5b9e\u9a8c\u914d\u7f6e (Setup)", "text": "<p>\u4e3a\u4e86\u5e94\u5bf9\u81ea\u7136\u573a\u666f\u7684\u7eb9\u7406\u590d\u6742\u6027\uff0c\u6211\u9009\u62e9\u4e86\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u66f4\u5f3a\u7684 ResNet50\uff0c\u5e76\u542f\u7528\u4e86 ImageNet \u9884\u8bad\u7ec3\u6743\u91cd\u3002</p> <ul> <li> <p>\u6a21\u578b: ResNet50 (<code>timm/resnet50.a1_in1k</code>) </p> </li> <li> <p>\u6570\u636e\u96c6: Intel Image Classification (6 Classes) </p> </li> <li> <p>\u4f18\u5316\u5668: AdamW</p> </li> <li> <p>\u8c03\u5ea6\u5668: Cosine Annealing (\u4f59\u5f26\u9000\u706b) </p> </li> <li> <p>\u65e9\u505c\u7b56\u7565: Patience = 7 (\u8010\u5fc3\u503c\u4e3a 7 \u8f6e) </p> </li> </ul>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#2", "title": "2. \u8bad\u7ec3\u8fc7\u7a0b\u5206\u6790\uff1a\u6781\u901f\u6536\u655b\u4e0e\u65e9\u505c", "text": "<p>\u8bad\u7ec3\u8fc7\u7a0b\u975e\u5e38\u987a\u5229\uff0c\u751a\u81f3\u53ef\u4ee5\u7528\u201c\u60ca\u4eba\u201d\u6765\u5f62\u5bb9\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u7387\uff1a</p> <ul> <li> <p>\u8d77\u6b65\u5373\u5dc5\u5cf0\uff1a \u4ec5\u4ec5\u5728 Epoch 1\uff0c\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\uff08Val Acc\uff09\u5c31\u76f4\u63a5\u98d9\u5347\u5230\u4e86 92.37% \u3002\u8fd9\u8bf4\u660e ImageNet \u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u5305\u542b\u7684\u201c\u5c71\u3001\u6c34\u3001\u5efa\u7b51\u201d\u7279\u5f81\u53ef\u4ee5\u5b8c\u7f8e\u8fc1\u79fb\u5230\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a\u3002</p> </li> <li> <p>\u6700\u4f73\u65f6\u523b\uff1a \u6a21\u578b\u5728 Epoch 4 \u8fbe\u5230\u4e86\u5168\u573a\u6700\u4f73\u6027\u80fd\uff0c\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387 93.43%\uff0cLoss \u964d\u81f3 0.1864 \u3002</p> </li> <li> <p>\u65e9\u505c\u89e6\u53d1\uff1a \u4ece Epoch 5 \u5f00\u59cb\uff0c\u867d\u7136\u8bad\u7ec3\u96c6\u51c6\u786e\u7387\u4e00\u8def\u51b2\u9ad8\u81f3 97.45%\uff0c\u4f46\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u4e00\u76f4\u5728 92.8% - 93.3% \u4e4b\u95f4\u5f98\u5f8a\uff0c\u751a\u81f3 Loss \u5f00\u59cb\u8f7b\u5fae\u53cd\u5f39 \u3002 \u6700\u7ec8\u5728 Epoch 11\uff0c\u7531\u4e8e\u8fde\u7eed 7 \u8f6e\u6ca1\u6709\u7a81\u7834 Epoch 4 \u7684\u8bb0\u5f55\uff0c\u65e9\u505c\u673a\u5236 (Early Stopping) \u89e6\u53d1\uff0c\u81ea\u52a8\u7ec8\u6b62\u4e86\u8bad\u7ec3 \u3002\u8fd9\u6210\u529f\u5e2e\u6211\u8282\u7701\u4e86\u7b97\u529b\uff0c\u5e76\u9632\u6b62\u4e86\u8fdb\u4e00\u6b65\u7684\u8fc7\u62df\u5408\u3002</p> </li> </ul> <p></p>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#3", "title": "3. \u7ed3\u679c\u6df1\u5ea6\u8bc4\u4f30", "text": "<p>\u6700\u7ec8\u5728\u6d4b\u8bd5\u96c6\uff08\u590d\u7528\u7684 Val \u96c6\uff09\u4e0a\u7684\u603b\u4f53\u8bc4\u4f30\u7ed3\u679c\u5982\u4e0b\uff1a</p> <ul> <li> <p>Accuracy: 93.43% </p> </li> <li> <p>Weighted F1-Score: 0.9339 </p> </li> </ul>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#31", "title": "3.1 \u8c01\u8868\u73b0\u6700\u597d\uff1f", "text": "<ul> <li> <p>Forest (\u68ee\u6797)\uff1a\u6a21\u578b\u7b80\u76f4\u662f\u201c\u4e00\u773c\u8fa8\u771f\u201d\uff0cRecall \u9ad8\u8fbe 99.37%\uff0c\u51e0\u4e4e\u6ca1\u6709\u6f0f\u7f51\u4e4b\u9c7c \u3002</p> </li> <li> <p>Sea (\u6d77\u6d0b)\uff1a\u8868\u73b0\u540c\u6837\u5f3a\u608d\uff0cRecall \u8fbe\u5230 99.41% \u3002</p> </li> </ul> <p>\u5206\u6790\uff1a\u8fd9\u4e24\u7c7b\u7279\u5f81\u6781\u5176\u660e\u663e\uff08\u7eff\u8272\u7684\u7eb9\u7406\u3001\u84dd\u8272\u7684\u6c34\u9762\uff09\uff0c\u6a21\u578b\u5b66\u5f97\u975e\u5e38\u5feb\u3002</p>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#32", "title": "3.2 \u8c01\u5728\u201c\u6253\u67b6\u201d\uff1f\uff08\u6df7\u6dc6\u77e9\u9635\u5206\u6790\uff09", "text": "<p>\u867d\u7136\u603b\u4f53\u5206\u9ad8\uff0c\u4f46\u67e5\u770b\u6df7\u6dc6\u77e9\u9635\u53ef\u4ee5\u53d1\u73b0\u4e24\u4e2a\u660e\u663e\u7684\u201c\u91cd\u707e\u533a\u201d\uff1a</p> <p>1. \u51b0\u5ddd (Glacier) vs. \u5c71\u8109 (Mountain) \u2014\u2014 \u6700\u5927\u7684\u96be\u70b9</p> <ul> <li>43 \u5f20 \u51b0\u5ddd\u88ab\u8bef\u8ba4\u6210\u4e86\u5c71\u8109\u3002</li> <li>48 \u5f20 \u5c71\u8109\u88ab\u8bef\u8ba4\u6210\u4e86\u51b0\u5ddd\u3002</li> <li>\u539f\u56e0\u5206\u6790\uff1a\u8fd9\u662f\u672c\u6570\u636e\u96c6\u6700\u7ecf\u5178\u7684\u96be\u9898\u3002\u5e26\u96ea\u7684\u5c71\u5cf0\uff08Mountain\uff09\u548c\u79ef\u96ea\u7684\u51b0\u5ddd\uff08Glacier\uff09\u5728\u89c6\u89c9\u7279\u5f81\u4e0a\u6781\u5ea6\u76f8\u4f3c\uff08\u90fd\u662f\u767d\u8272\u3001\u5ca9\u77f3\u8d28\u611f\uff09\uff0cResNet50 \u5728\u8fd9\u91cc\u4e5f\u72af\u4e86\u8ff7\u7cca\uff0c\u5bfc\u81f4\u8fd9\u4e24\u7c7b\u7684 Recall \u53ea\u6709 88% \u5de6\u53f3 \u3002</li> </ul> <p>2. \u5efa\u7b51 (Buildings) vs. \u8857\u9053 (Street)</p> <ul> <li>27 \u5f20 \u5efa\u7b51\u88ab\u8bef\u8ba4\u6210\u4e86\u8857\u9053\u3002</li> <li>23 \u5f20 \u8857\u9053\u88ab\u8bef\u8ba4\u6210\u4e86\u5efa\u7b51\u3002</li> <li>\u539f\u56e0\u5206\u6790\uff1a\u8fd9\u4e24\u7c7b\u90fd\u5c5e\u4e8e\u201c\u57ce\u5e02\u666f\u89c2\u201d\u3002\u8857\u9053\u56fe\u7247\u4e2d\u5f80\u5f80\u4e5f\u5305\u542b\u4e24\u65c1\u7684\u5efa\u7b51\u7269\uff0c\u6a21\u578b\u53ef\u80fd\u4f1a\u5173\u6ce8\u5230\u56fe\u7247\u8fb9\u7f18\u7684\u697c\u623f\u7279\u5f81\u800c\u8bef\u5224\u3002</li> </ul> <p></p>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#4", "title": "4. \u603b\u7ed3", "text": "<p>\u8fd9\u6b21\u5b9e\u9a8c\u8bc1\u660e\u4e86 ResNet50 + \u8fc1\u79fb\u5b66\u4e60 \u662f\u89e3\u51b3\u4e2d\u7b49\u96be\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u201c\u5927\u6740\u5668\u201d\u3002\u5728\u4e0d\u9700\u8981\u7e41\u7410\u8c03\u53c2\u7684\u60c5\u51b5\u4e0b\uff0c4 \u4e2a Epoch \u5c31\u80fd\u8fbe\u5230 93% \u7684\u5de5\u4e1a\u53ef\u7528\u6c34\u5e73\u3002</p>"}, {"location": "cv/exercises/resnet50_Inter_Image_Classification/#5", "title": "5. \u4ee3\u7801", "text": "<p>\u672c\u6b21\u5b9e\u9a8c\u57fa\u4e8e\u9884\u5148\u6784\u5efa\u7684\u56fe\u50cf\u5206\u7c7b\u901a\u7528\u6846\u67b6\u8fdb\u884c\uff0c\u6b64\u5904\u4ec5\u5bf9\u5dee\u5f02\u5316\u7684\u5fae\u8c03\u90e8\u5206\uff08Fine-tuning\uff09\u8fdb\u884c\u8bf4\u660e\u3002</p> <pre><code>class Config:\n    # --- \u6570\u636e\u96c6\u8bbe\u7f6e ---\n    USE_CUSTOM_DATASET = True  # &lt;--- [\u53ef\u5fae\u8c03] True=\u81ea\u5b9a\u4e49\u6587\u4ef6\u5939, False=\u5185\u7f6e\n    CUSTOM_DATA_ROOT = \"./datasets/Intel_Image_Classification\"  # &lt;--- [\u53ef\u5fae\u8c03] \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8def\u5f84\n    BUILTIN_NAME = \"\"  # &lt;--- [\u53ef\u5fae\u8c03] \u5185\u7f6e\u6570\u636e\u96c6\u540d\u79f0\n    DATA_DOWNLOAD_ROOT = \"./data\"  # &lt;--- [\u53ef\u5fae\u8c03] \u4e0b\u8f7d\u7f13\u5b58\u8def\u5f84\n\n    # --- \u7ed3\u679c\u4fdd\u5b58 ---\n    SAVE_DIR_ROOT = \"./results\"  # &lt;--- [\u53ef\u5fae\u8c03] \u7ed3\u679c\u4fdd\u5b58\u6839\u76ee\u5f55\n    SAVE_DIR = \"\"  # (\u8fd0\u884c\u65f6\u81ea\u52a8\u751f\u6210)\n\n    # --- \u6a21\u578b\u8bbe\u7f6e ---\n    MODEL_NAME = \"resnet50\"  # &lt;--- [\u53ef\u5fae\u8c03] \u6a21\u578b\u540d\u79f0 (timm\u5e93\u652f\u6301\u7684\u540d\u79f0)\n    CHECKPOINT_PATH = \"\"  # &lt;--- [\u53ef\u5fae\u8c03] \u521d\u59cb\u9884\u8bad\u7ec3\u6743\u91cd (\u8fc1\u79fb\u5b66\u4e60\u7528)\n    RESUME_PATH = \"\"  # &lt;--- [\u53ef\u5fae\u8c03] \u65ad\u70b9\u7eed\u8bad\u6587\u4ef6\u8def\u5f84 (.pth)\n    NUM_CLASSES = 0  # (\u8fd0\u884c\u65f6\u81ea\u52a8\u8986\u76d6)\n\n    # --- \u8bad\u7ec3\u8d85\u53c2\u6570 ---\n    BATCH_SIZE = 16  # &lt;--- [\u53ef\u5fae\u8c03] \u6279\u6b21\u5927\u5c0f\n    EPOCHS = 50  # &lt;--- [\u53ef\u5fae\u8c03] \u8bad\u7ec3\u603b\u8f6e\u6570\n    LR = 1e-4  # &lt;--- [\u53ef\u5fae\u8c03] \u521d\u59cb\u5b66\u4e60\u7387\n    WEIGHT_DECAY = 1e-4  # &lt;--- [\u53ef\u5fae\u8c03] L2\u6b63\u5219\u5316\u7cfb\u6570\n    SEED = 42  # &lt;--- [\u53ef\u5fae\u8c03] \u968f\u673a\u79cd\u5b50\n\n    # --- \u7b56\u7565\u9009\u62e9 ---\n    OPTIMIZER_NAME = 'adamw'  # &lt;--- [\u53ef\u5fae\u8c03] 'adamw', 'adam', 'sgd'\n    SCHEDULER_NAME = 'cosine'  # &lt;--- [\u53ef\u5fae\u8c03] 'plateau', 'cosine', 'step'\n\n    # --- \u65e9\u505c\u8bbe\u7f6e ---\n    # 0 \u6216 None \u8868\u793a\u5173\u95ed\u65e9\u505c\uff0c&gt; 0 \u8868\u793a\u5f00\u542f\u65e9\u505c\u7684\u8010\u5fc3\u8f6e\u6570\n    EARLY_STOP_PATIENCE = 7  # &lt;--- [\u53ef\u5fae\u8c03] \u65e9\u505c\u8010\u5fc3\u8f6e\u6570 (0=\u5173\u95ed)\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"}]}