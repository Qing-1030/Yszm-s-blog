# ResNet50 木薯叶病害分类

在完成了基础图像分类任务后，我将目光投向了更具挑战性的 Kaggle 竞赛数据集——**Cassava Leaf Disease Classification（木薯叶病害分类）**。这是一个典型的细粒度分类任务，且以严重的**类别不平衡**著称。

本次实验我使用了经典的 **ResNet50** 模型，最终在验证集上达到了 **85.19%** 的准确率。虽然成绩尚可，但通过分析训练日志和混淆矩阵，我发现了许多值得深挖的“隐患”。以下是我的详细复盘。

## 1. 实验配置

* **模型架构**: ResNet50 (预训练权重: `timm/resnet50.a1_in1k`)
* **数据集**: 5 分类 (CBB, CBSD, CGM, CMD, Healthy)
* **优化器**: AdamW
* **调度器**: Cosine Annealing (余弦退火)
* **早停策略**: Patience = 7 (耐心值 7 轮)

## 2. 训练过程：起步即巅峰，随后过拟合

训练过程非常迅速，ResNet50 展现了强大的特征提取能力，但也很快触碰到了瓶颈。

* **极速收敛**：
仅仅在 **Epoch 1**，验证集准确率就达到了 **75.65%** 。这得益于 ImageNet 预训练权重对自然纹理的良好适应性。


* **黄金时刻 (Epoch 7)**：
模型在 **Epoch 7** 达到了全场最佳性能，验证集准确率 **85.19%**，Loss 降至最低点 **0.3787** 。


* **过拟合的信号**：
从曲线图中可以明显看到，在 Epoch 7 之后，**蓝色线（训练集准确率）** 一路高歌猛进，最终在 Epoch 14 达到了 **92.29%** ；然而，**橙色线（验证集准确率）** 却开始停滞甚至下滑，最终停留在 **83.60%** 。
同时，验证集的 Loss 在 Epoch 10 之后开始反弹（从 0.30 涨到 0.54），这是典型的**过拟合 (Overfitting)** 征兆。最终，早停机制在 Epoch 14 触发，终止了训练 。

![训练曲线](resnet50_Cassava_leaf_Diease_Classification_training_curve.png)

## 3. 结果深度剖析：被“大类”统治的准确率

最终的评估结果定格在 **85.19%** 。乍一看这个分数还不错，但如果我们拆解开来看，就会发现严重的问题。

### 3.1 严重的类别不平衡

查看分类报告 (Classification Report)，我们可以看到样本分布极其不均：

* **3_CMD (花叶病)**: 样本数高达 **1316 张** 。

* **0_CBB (细菌性枯萎病)**: 样本数仅有 **109 张** 。

* **差距**: 类别 3 的数据量是类别 0 的 **12 倍**！

### 3.2 混淆矩阵说了什么？

混淆矩阵直观地展示了这种不平衡带来的后果：

1. **“学霸”类别 (3_CMD)**：
由于数据量巨大，模型在这个类别上表现极好，Recall 高达 **94.83%** 。混淆矩阵中右下角那个深蓝色的方块 (1248 张正确预测) 就是证明。

2. **“学渣”类别 (0_CBB)**：
由于样本太少，模型根本学不会。109 张图里，只有 **67 张** 预测正确，Recall 仅为 **61.47%** 。大量的 CBB 图片被误判为了 Healthy (24张) 或 CMD (8张)。

3. **“和稀泥”现象**：
观察混淆矩阵的第四列（3_CMD），你会发现其他所有类别都有大量图片被误判为 3_CMD。
* **2_CGM**: 有 40 张被误判为 CMD。
* **4_Healthy**: 有 27 张被误判为 CMD。
* **原因**：模型为了刷高总准确率，倾向于将模棱两可的图片直接猜成数量最多的类别（CMD），这是一种投机取巧的策略。

![混淆矩阵](resnet50_Cassava_leaf_Diease_Classification_confusion_matrix.png)

## 4. 总结与改进方向

这次实验证明了 ResNet50 在数据量充足的情况下（如类别 3）能达到极高的精度（F1-score 0.94），但在小样本类别上表现惨淡（F1-score 0.62）。**85.19% 的总准确率在一定程度上是靠“猜大类”撑起来的虚高分数。**

**接下来的优化清单：**

1. **引入 Class Weights (类别权重)**：在计算 Loss 时，给数量少的类别（如 0_CBB）更高的惩罚权重，强迫模型去关注小样本。
2. **使用 Mixup 或 CutMix**：这是解决 Cassava 数据集过拟合和提高泛化能力的神器。通过混合图片和标签，让模型不再“死记硬背”。
3. **尝试 Focal Loss**：降低易分类样本的权重，专注于那些难分类的样本。

这次训练虽然止步于 85%，但它清晰地暴露了不平衡数据集的痛点，为下一步的进阶优化指明了方向！

## 5. 代码

本次实验基于预先构建的图像分类通用框架进行，此处仅对差异化的微调部分（Fine-tuning）进行说明。

```python
class Config:
    # --- 数据集路径设置 ---
    USE_CUSTOM_DATASET = True  # [必改] True=使用自定义文件夹, False=使用内置(CIFAR10等)
    CUSTOM_DATA_ROOT = "./datasets/Cassava_Leaf_Disease_Classification"  # [必改] 自定义数据集根目录 (包含 train/val)
    BUILTIN_NAME = ""  # [可选] 内置数据集名称 (仅当上面为False时生效)
    DATA_DOWNLOAD_ROOT = "./data"  # [可选] 数据下载缓存路径

    # --- 结果保存设置 ---
    SAVE_DIR_ROOT = "./results"  # [可选] 训练结果保存根目录
    SAVE_DIR = ""  # (程序自动生成，无需修改)

    # --- 模型与训练设置 ---
    MODEL_NAME = "resnet50"  # [可选] 模型名称 (如 resnet50, efficientnet_b0)
    CHECKPOINT_PATH = ""  # [可选] 预训练权重路径 (空则下载ImageNet权重)
    RESUME_PATH = ""  # [可选] 断点续训的 .pth 文件路径
    NUM_CLASSES = 0  # (程序自动识别，无需修改)

    # --- 超参数设置 ---
    BATCH_SIZE = 16  # [微调] 批次大小 (显存不足改小)
    EPOCHS = 50  # [微调] 训练轮数
    LR = 1e-4  # [微调] 初始学习率 (微调通常用 1e-4 或 1e-5)
    WEIGHT_DECAY = 1e-4  # [微调] 正则化系数 (抗过拟合用)
    SEED = 42  # [可选] 随机种子 (保证结果可复现)

    # --- 优化策略 ---
    OPTIMIZER_NAME = 'adamw'  # [可选] 优化器: 'adamw', 'adam', 'sgd'
    SCHEDULER_NAME = 'cosine'  # [可选] 学习率策略: 'plateau'(监控), 'cosine'(余弦), 'step'

    # --- 早停 (Early Stopping) ---
    EARLY_STOP_PATIENCE = 7  # [可选] 连续多少轮不涨分就停止 (0表示关闭)

    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```