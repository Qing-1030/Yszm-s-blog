---
title: å›¾åƒåˆ†ç±»æ¨¡å‹é€šç”¨æ¨¡æ¿
date: 2026-01-12
tags:
  - Python
  - è®¡ç®—æœºè§†è§‰
  - å›¾åƒåˆ†ç±»
---

# å›¾åƒåˆ†ç±»æ¨¡å‹é€šç”¨æ¨¡æ¿

> æ‘˜è¦ï¼šä¸€ä¸ªå¯é…ç½®çš„ä»£ç æ¨¡æ¿ï¼Œç”¨äºè®­ç»ƒå›¾åƒåˆ†ç±»æ¨¡å‹

## æ ¸å¿ƒä»£ç ï¼š

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import timm
from timm.data import resolve_data_config
from tqdm import tqdm
import matplotlib.pyplot as plt

# å°è¯•å¯¼å…¥é«˜çº§è¯„ä¼°åº“
try:
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("[Warning] æœªå®‰è£… sklearn æˆ– seabornï¼Œè·³è¿‡æ··æ·†çŸ©é˜µç»˜åˆ¶ã€‚")

# ==========================================
# 1. å…¨å±€é…ç½®åŒºåŸŸ
# ==========================================
class Config:
    # --- æ•°æ®é›†è®¾ç½® ---
    # True = ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶å¤¹ (éœ€åŒ…å« train/val æ–‡ä»¶å¤¹)
    # False = ä½¿ç”¨ PyTorch æ ‡å‡†å†…ç½®æ•°æ®é›† (è‡ªåŠ¨ä¸‹è½½)
    USE_CUSTOM_DATASET = True       # <--- [å¯å¾®è°ƒ] æ•°æ®é›†æ¨¡å¼å¼€å…³
    
    # æ¨¡å¼1ï¼šè‡ªå®šä¹‰æ–‡ä»¶å¤¹è·¯å¾„
    CUSTOM_DATA_ROOT = "flower_data" # <--- [å¯å¾®è°ƒ] ä½ çš„æ•°æ®é›†æ–‡ä»¶å¤¹è·¯å¾„
    
    # æ¨¡å¼2ï¼šå†…ç½®æ•°æ®é›†åç§° (å¦‚ CIFAR10, CIFAR100, FashionMNIST)
    BUILTIN_NAME = "CIFAR10"        # <--- [å¯å¾®è°ƒ] å†…ç½®æ•°æ®é›†åç§°
    DATA_DOWNLOAD_ROOT = "./data"   # <--- [å¯å¾®è°ƒ] æ•°æ®ä¸‹è½½è·¯å¾„
    
    # --- ç»“æœä¿å­˜ ---
    SAVE_DIR = "./results"          # <--- [å¯å¾®è°ƒ] è®­ç»ƒç»“æœ/æ¨¡å‹ä¿å­˜è·¯å¾„
    
    # --- æ¨¡å‹è®¾ç½® ---
    # æ¨èæ¨¡å‹: resnet50, resnet18, efficientnet_b0, inception_v3.tf_in1k
    MODEL_NAME = "resnet50"         # <--- [å¯å¾®è°ƒ] ä½¿ç”¨çš„æ¨¡å‹åç§°
    
    # æœ¬åœ°æƒé‡è·¯å¾„è®¾ç½®ï¼š
    # 1. å¡«å…¥å…·ä½“è·¯å¾„ (å¦‚ "resnet50.bin") -> å¼ºåˆ¶åŠ è½½æœ¬åœ°æƒé‡ (pretrained=False)
    # 2. ç•™ç©º "" -> è‡ªåŠ¨ä»ç½‘ç»œä¸‹è½½é¢„è®­ç»ƒæƒé‡ (pretrained=True)
    CHECKPOINT_PATH = ""            # <--- [å¯å¾®è°ƒ] æœ¬åœ°é¢„è®­ç»ƒæƒé‡è·¯å¾„
    
    # åˆå§‹ç±»åˆ«æ•° (ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹çœŸå®ç±»åˆ«æ•°å¹¶è¦†ç›–æ­¤å€¼)
    NUM_CLASSES = 5                 
    
    # --- è®­ç»ƒè¶…å‚æ•° ---
    BATCH_SIZE = 32                 # <--- [å¯å¾®è°ƒ] æ‰¹æ¬¡å¤§å° (æ˜¾å­˜ä¸è¶³å¯è°ƒå°)
    EPOCHS = 20                     # <--- [å¯å¾®è°ƒ] è®­ç»ƒæ€»è½®æ•°
    LR = 1e-4                       # <--- [å¯å¾®è°ƒ] åˆå§‹å­¦ä¹ ç‡
    WEIGHT_DECAY = 1e-4             # <--- [å¯å¾®è°ƒ] L2æ­£åˆ™åŒ–ç³»æ•°
    
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# 2. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½
# ==========================================
def get_transforms(model_cfg):
    """æ ¹æ®æ¨¡å‹é…ç½®è‡ªåŠ¨ç”Ÿæˆé¢„å¤„ç†æµç¨‹"""
    # è‡ªåŠ¨è·å–æ¨¡å‹æ‰€éœ€çš„è¾“å…¥å°ºå¯¸
    input_size = model_cfg.get('input_size', (3, 224, 224))
    crop_size = input_size[1] 
    
    # è·å–æ¨¡å‹ç‰¹å®šçš„å‡å€¼å’Œæ–¹å·®
    mean = model_cfg.get('mean', [0.485, 0.456, 0.406])
    std = model_cfg.get('std', [0.229, 0.224, 0.225])
    
    print(f"[Info] é¢„å¤„ç†é…ç½®: Size={crop_size}, Mean={mean}, Std={std}")

    # è®­ç»ƒé›†å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((crop_size, crop_size)),  # ç»Ÿä¸€å›¾åƒå°ºå¯¸
        transforms.RandomHorizontalFlip(0.5),       # <--- [å¯å¾®è°ƒ] éšæœºæ°´å¹³ç¿»è½¬æ¦‚ç‡
        transforms.RandomRotation(15),              # <--- [å¯å¾®è°ƒ] éšæœºæ—‹è½¬è§’åº¦
        transforms.ColorJitter(brightness=0.1, contrast=0.1), # <--- [å¯å¾®è°ƒ] é¢œè‰²/å¯¹æ¯”åº¦æ‰°åŠ¨
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])

    # éªŒè¯/æµ‹è¯•é›†å¤„ç† (ä»…æ ‡å‡†åŒ–)
    val_test_transform = transforms.Compose([
        transforms.Resize((crop_size, crop_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    
    return train_transform, val_test_transform

def get_dataloaders(train_tf, val_tf):
    """æ ¹æ®é…ç½®åŠ è½½å†…ç½®æˆ–è‡ªå®šä¹‰æ•°æ®é›†"""
    train_ds, val_ds, test_ds = None, None, None
    class_names = []
    
    # --- åˆ†æ”¯ 1: ä½¿ç”¨å†…ç½®æ•°æ®é›† ---
    if not Config.USE_CUSTOM_DATASET:
        print(f"[Data] åŠ è½½å†…ç½®æ•°æ®é›†: {Config.BUILTIN_NAME}")
        try:
            DatasetClass = getattr(datasets, Config.BUILTIN_NAME)
        except AttributeError:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {Config.BUILTIN_NAME}")

        # åŠ è½½å®Œæ•´æ•°æ®é›†
        full_train_ds = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=True, download=True, transform=train_tf)
        test_ds       = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=False, download=True, transform=val_tf)
        
        # è‡ªåŠ¨åˆ’åˆ†éªŒè¯é›† (90% è®­ç»ƒ, 10% éªŒè¯)
        train_size = int(0.9 * len(full_train_ds)) # <--- [å¯å¾®è°ƒ] éªŒè¯é›†æ¯”ä¾‹
        val_size = len(full_train_ds) - train_size
        train_ds, val_ds = random_split(full_train_ds, [train_size, val_size])
        
        class_names = full_train_ds.classes

    # --- åˆ†æ”¯ 2: ä½¿ç”¨è‡ªå®šä¹‰æ–‡ä»¶å¤¹ ---
    else:
        print(f"[Data] åŠ è½½è‡ªå®šä¹‰æ–‡ä»¶å¤¹: {Config.CUSTOM_DATA_ROOT}")
        train_dir = os.path.join(Config.CUSTOM_DATA_ROOT, "train")
        val_dir   = os.path.join(Config.CUSTOM_DATA_ROOT, "val")
        test_dir  = os.path.join(Config.CUSTOM_DATA_ROOT, "test")
        
        if not os.path.exists(train_dir):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è®­ç»ƒç›®å½•: {train_dir}")

        train_ds = datasets.ImageFolder(train_dir, transform=train_tf)
        val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æµ‹è¯•é›†
        if os.path.exists(test_dir):
            test_ds = datasets.ImageFolder(test_dir, transform=val_tf)
        else:
            print("[Info] æœªæ‰¾åˆ° test æ–‡ä»¶å¤¹ï¼Œè·³è¿‡æµ‹è¯•æ­¥éª¤ã€‚")
            
        class_names = train_ds.classes

    # æ›´æ–°å…¨å±€ç±»åˆ«æ•°
    Config.NUM_CLASSES = len(class_names)
    print(f"[Data] æ£€æµ‹åˆ° {Config.NUM_CLASSES} ä¸ªç±»åˆ«: {class_names}")
    
    # åˆ›å»ºDataLoader
    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader   = DataLoader(val_ds,   batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)
    test_loader  = DataLoader(test_ds,  batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2) if test_ds else None
    
    return train_loader, val_loader, test_loader, class_names

# ==========================================
# 3. è®­ç»ƒä¸éªŒè¯é€»è¾‘
# ==========================================
def train_one_epoch(model, loader, criterion, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªEpoch"""
    model.train()
    total_loss, total_correct = 0.0, 0
    
    bar = tqdm(loader, desc=f"Epoch {epoch}/{Config.EPOCHS} [Train]")
    for imgs, labels in bar:
        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)
        
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * imgs.size(0)
        total_correct += (outputs.argmax(1) == labels).sum().item()
        bar.set_postfix(loss=loss.item())
        
    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)

@torch.no_grad()
def validate(model, loader, criterion, epoch, phase="Val"):
    """éªŒè¯æ¨¡å‹æ€§èƒ½"""
    model.eval()
    total_loss, total_correct = 0.0, 0
    
    bar = tqdm(loader, desc=f"Epoch {epoch}/{Config.EPOCHS} [{phase}]  ")
    for imgs, labels in bar:
        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        
        total_loss += loss.item() * imgs.size(0)
        total_correct += (outputs.argmax(1) == labels).sum().item()
        bar.set_postfix(loss=loss.item())
        
    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)

def evaluate_test_set(model, test_loader, class_names):
    """æµ‹è¯•é›†è¯¦ç»†è¯„ä¼°ï¼šç”ŸæˆæŠ¥å‘Šä¸æ··æ·†çŸ©é˜µ"""
    if not test_loader: return
    if not HAS_SKLEARN: return

    model.eval()
    all_preds = []
    all_labels = []
    
    print(f"\n[Test] æ­£åœ¨è¿›è¡Œæœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°...")
    with torch.no_grad():
        for imgs, labels in tqdm(test_loader, desc="Testing"):
            imgs = imgs.to(Config.DEVICE), labels.to(Config.DEVICE) # ç¡®ä¿æ•°æ®åœ¨åŒä¸€è®¾å¤‡
            imgs = imgs[0] # è§£åŒ…
            labels = labels[0]

            outputs = model(imgs)
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # æ‰“å°åˆ†ç±»æŠ¥å‘Š
    print("\n" + "="*50)
    print("FINAL TEST REPORT")
    print("="*50)
    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    cm = confusion_matrix(all_labels, all_preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    save_path = os.path.join(Config.SAVE_DIR, 'confusion_matrix.png')
    plt.savefig(save_path)
    print(f"[Info] æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {save_path}")

def plot_history(history, save_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    epochs = range(1, len(history['train_acc']) + 1)
    plt.figure(figsize=(12, 5))
    
    # å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_acc'], 'b-o', label='Train Acc')
    plt.plot(epochs, history['val_acc'], 'r-o', label='Val Acc')
    plt.title('Accuracy'); plt.legend()
    
    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_loss'], 'b-o', label='Train Loss')
    plt.plot(epochs, history['val_loss'], 'r-o', label='Val Loss')
    plt.title('Loss'); plt.legend()
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'training_curve.png'))
    print(f"[Info] è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³: {save_dir}")

# ==========================================
# 4. ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    os.makedirs(Config.SAVE_DIR, exist_ok=True)
    
    # --- 1. è·å–æ¨¡å‹é»˜è®¤é…ç½® ---
    print(f"[Init] è·å– {Config.MODEL_NAME} é»˜è®¤é…ç½®...")
    temp_model = timm.create_model(Config.MODEL_NAME, pretrained=True)
    model_cfg = resolve_data_config({}, model=temp_model)
    del temp_model 
    
    # --- 2. å‡†å¤‡æ•°æ® ---
    train_tf, val_test_tf = get_transforms(model_cfg)
    train_loader, val_loader, test_loader, class_names = get_dataloaders(train_tf, val_test_tf)
    
    # --- 3. åˆå§‹åŒ–æ¨¡å‹ (æ ¸å¿ƒä¿®æ”¹ç‚¹) ---
    print(f"[Init] åˆ›å»ºæ¨¡å‹: {Config.MODEL_NAME}")
    
    # åˆ¤æ–­æ˜¯å¦åŠ è½½æœ¬åœ°æƒé‡
    if Config.CHECKPOINT_PATH and os.path.exists(Config.CHECKPOINT_PATH):
        print(f"[Load] åŠ è½½æœ¬åœ°æƒé‡: {Config.CHECKPOINT_PATH}")
        model = timm.create_model(
            Config.MODEL_NAME,
            pretrained=False,                       # å…³é—­è‡ªåŠ¨ä¸‹è½½
            checkpoint_path=Config.CHECKPOINT_PATH  # æŒ‡å®šæœ¬åœ°è·¯å¾„
        )
    else:
        print(f"[Load] ä½¿ç”¨åœ¨çº¿é¢„è®­ç»ƒæƒé‡ (pretrained=True)")
        model = timm.create_model(
            Config.MODEL_NAME,
            pretrained=True                         # å¼€å¯è‡ªåŠ¨ä¸‹è½½
        )
    
    # é‡ç½®åˆ†ç±»å¤´ä»¥åŒ¹é…å½“å‰æ•°æ®ç±»åˆ«
    print(f"[Init] é‡ç½®åˆ†ç±»å¤´ä¸º {Config.NUM_CLASSES} ç±»")
    model.reset_classifier(num_classes=Config.NUM_CLASSES)
    model.to(Config.DEVICE)
    
    # --- 4. è®­ç»ƒå¾ªç¯ ---
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)
    
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    best_acc = 0.0
    
    print(f"\n[Start] å¼€å§‹è®­ç»ƒ... (è®¾å¤‡: {Config.DEVICE})")
    for epoch in range(1, Config.EPOCHS + 1):
        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, epoch)
        v_loss, v_acc = validate(model, val_loader, criterion, epoch)
        
        # è®°å½•æ—¥å¿—
        history['train_loss'].append(t_loss)
        history['train_acc'].append(t_acc)
        history['val_loss'].append(v_loss)
        history['val_acc'].append(v_acc)
        
        print(f"Epoch {epoch}: Train Acc: {t_acc:.4f} | Val Acc: {v_acc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if v_acc > best_acc:
            best_acc = v_acc
            torch.save(model.state_dict(), os.path.join(Config.SAVE_DIR, "best_model.pth"))
            print(f" -> ğŸŒŸ æœ€ä½³æ¨¡å‹å·²æ›´æ–° (Acc: {best_acc:.4f})")
            
    # --- 5. ç»“æœå¯è§†åŒ–ä¸æµ‹è¯• ---
    plot_history(history, Config.SAVE_DIR)
    
    if test_loader:
        print("[Info] åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        model.load_state_dict(torch.load(os.path.join(Config.SAVE_DIR, "best_model.pth")))
        evaluate_test_set(model, test_loader, class_names)
        
    print("\n[Done] å…¨éƒ¨å®Œæˆï¼")
```