---
title: å›¾åƒåˆ†ç±»æ¨¡å‹é€šç”¨æ¨¡æ¿
date: 2026-01-12
tags:
  - Python
  - è®¡ç®—æœºè§†è§‰
  - å›¾åƒåˆ†ç±»
---

# å›¾åƒåˆ†ç±»æ¨¡å‹é€šç”¨æ¨¡æ¿

> æ‘˜è¦ï¼šä¸€ä¸ªå¯é…ç½®çš„ä»£ç æ¨¡æ¿ï¼Œç”¨äºè®­ç»ƒå›¾åƒåˆ†ç±»æ¨¡å‹

## æ ¸å¿ƒä»£ç ï¼š

```python
import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import timm
from timm.data import resolve_data_config
from tqdm import tqdm
import matplotlib.pyplot as plt

# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†é«˜çº§è¯„ä¼°åº“
try:
    from sklearn.metrics import classification_report, confusion_matrix
    import seaborn as sns
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("[Warning] æœªå®‰è£… sklearn æˆ– seabornï¼Œè·³è¿‡æ··æ·†çŸ©é˜µç»˜åˆ¶ã€‚")

# ==========================================
# 1. å…¨å±€é…ç½®åŒºåŸŸ
# ==========================================
class Config:
    # --- æ•°æ®é›†è®¾ç½® ---
    USE_CUSTOM_DATASET = True        # <--- [å¯å¾®è°ƒ] True=è‡ªå®šä¹‰æ–‡ä»¶å¤¹, False=å†…ç½®æ•°æ®é›†
    CUSTOM_DATA_ROOT = "flower_data" # <--- [å¯å¾®è°ƒ] è‡ªå®šä¹‰æ•°æ®é›†æ ¹ç›®å½•
    BUILTIN_NAME = "CIFAR10"         # <--- [å¯å¾®è°ƒ] å†…ç½®æ•°æ®é›†åç§° (å¦‚ CIFAR10, CIFAR100)
    DATA_DOWNLOAD_ROOT = "./data"    # <--- [å¯å¾®è°ƒ] æ•°æ®é›†ä¸‹è½½ç¼“å­˜è·¯å¾„
    
    # --- ç»“æœä¿å­˜ ---
    SAVE_DIR_ROOT = "./results"      # <--- [å¯å¾®è°ƒ] ç»“æœä¿å­˜æ ¹ç›®å½• (ä¼šè‡ªåŠ¨ç”Ÿæˆå­æ–‡ä»¶å¤¹)
    SAVE_DIR = ""                    # (è¿è¡Œæ—¶è‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€ä¿®æ”¹)
    
    # --- æ¨¡å‹è®¾ç½® ---
    MODEL_NAME = "resnet50"          # <--- [å¯å¾®è°ƒ] æ¨¡å‹åç§° (timmåº“æ”¯æŒçš„åç§°)
    
    # æœ¬åœ°é¢„è®­ç»ƒæƒé‡è·¯å¾„
    # "" (ç©ºå­—ç¬¦ä¸²) = è‡ªåŠ¨ä¸‹è½½åœ¨çº¿æƒé‡
    # "xxx.bin"     = å¼ºåˆ¶åŠ è½½æœ¬åœ°æ–‡ä»¶
    CHECKPOINT_PATH = ""             # <--- [å¯å¾®è°ƒ] åˆå§‹é¢„è®­ç»ƒæƒé‡è·¯å¾„
    
    # æ–­ç‚¹ç»­è®­è·¯å¾„
    # "" (ç©ºå­—ç¬¦ä¸²) = ä»å¤´å¼€å§‹è®­ç»ƒ
    # "./results/xxx/last.pth" = ä»æŒ‡å®šæ–­ç‚¹æ¢å¤
    RESUME_PATH = ""                 # <--- [å¯å¾®è°ƒ] æ–­ç‚¹ç»­è®­æ–‡ä»¶è·¯å¾„
    
    NUM_CLASSES = 0                  # (è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹è¦†ç›–)
    
    # --- è®­ç»ƒè¶…å‚æ•° ---
    BATCH_SIZE = 32                  # <--- [å¯å¾®è°ƒ] æ‰¹æ¬¡å¤§å°
    EPOCHS = 20                      # <--- [å¯å¾®è°ƒ] è®­ç»ƒæ€»è½®æ•°
    LR = 1e-4                        # <--- [å¯å¾®è°ƒ] åˆå§‹å­¦ä¹ ç‡
    WEIGHT_DECAY = 1e-4              # <--- [å¯å¾®è°ƒ] æƒé‡è¡°å‡ (L2æ­£åˆ™åŒ–)
    
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# 2. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½
# ==========================================
def get_transforms(model_cfg):
    # è¯»å–æ¨¡å‹å¯¹åº”çš„é»˜è®¤è¾“å…¥å°ºå¯¸å’Œå‡å€¼æ–¹å·®
    input_size = model_cfg.get('input_size', (3, 224, 224))
    crop_size = input_size[1]
    mean = model_cfg.get('mean', [0.485, 0.456, 0.406])
    std = model_cfg.get('std', [0.229, 0.224, 0.225])
    
    print(f"[Info] é¢„å¤„ç†å‚æ•°: Size={crop_size}, Mean={mean}, Std={std}")

    # è®­ç»ƒé›†å¢å¼ºç­–ç•¥
    train_tf = transforms.Compose([
        transforms.Resize((crop_size, crop_size)),
        transforms.RandomHorizontalFlip(0.5),                  # <--- [å¯å¾®è°ƒ] æ°´å¹³ç¿»è½¬æ¦‚ç‡
        transforms.RandomRotation(15),                         # <--- [å¯å¾®è°ƒ] æ—‹è½¬è§’åº¦
        transforms.ColorJitter(brightness=0.1, contrast=0.1),  # <--- [å¯å¾®è°ƒ] é¢œè‰²æ‰°åŠ¨
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    
    # éªŒè¯/æµ‹è¯•é›†ä»…åšæ ‡å‡†åŒ–
    val_test_tf = transforms.Compose([
        transforms.Resize((crop_size, crop_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean, std)
    ])
    return train_tf, val_test_tf

def get_dataloaders(train_tf, val_tf):
    if not Config.USE_CUSTOM_DATASET:
        # --- åŠ è½½å†…ç½®æ•°æ®é›† ---
        print(f"[Data] åŠ è½½å†…ç½®æ•°æ®é›†: {Config.BUILTIN_NAME}")
        try:
            DatasetClass = getattr(datasets, Config.BUILTIN_NAME)
        except AttributeError:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {Config.BUILTIN_NAME}")
            
        full_train_ds = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=True, download=True, transform=train_tf)
        test_ds = DatasetClass(root=Config.DATA_DOWNLOAD_ROOT, train=False, download=True, transform=val_tf)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (é»˜è®¤9:1)
        train_size = int(0.9 * len(full_train_ds))             # <--- [å¯å¾®è°ƒ] éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
        val_size = len(full_train_ds) - train_size
        train_ds, val_ds = random_split(full_train_ds, [train_size, val_size])
        
        class_names = full_train_ds.classes
        dataset_name = Config.BUILTIN_NAME
    else:
        # --- åŠ è½½è‡ªå®šä¹‰æ–‡ä»¶å¤¹ ---
        print(f"[Data] åŠ è½½è‡ªå®šä¹‰æ–‡ä»¶å¤¹: {Config.CUSTOM_DATA_ROOT}")
        train_dir = os.path.join(Config.CUSTOM_DATA_ROOT, "train")
        val_dir = os.path.join(Config.CUSTOM_DATA_ROOT, "val")
        test_dir = os.path.join(Config.CUSTOM_DATA_ROOT, "test")
        
        if not os.path.exists(train_dir): 
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç›®å½•: {train_dir}")
            
        train_ds = datasets.ImageFolder(train_dir, transform=train_tf)
        val_ds = datasets.ImageFolder(val_dir, transform=val_tf)
        test_ds = datasets.ImageFolder(test_dir, transform=val_tf) if os.path.exists(test_dir) else None
        
        class_names = train_ds.classes
        dataset_name = os.path.basename(Config.CUSTOM_DATA_ROOT)

    Config.NUM_CLASSES = len(class_names)
    print(f"[Data] ç±»åˆ«æ•°: {Config.NUM_CLASSES} -> {class_names}")
    
    # æ„å»º DataLoader
    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2) if test_ds else None
    
    return train_loader, val_loader, test_loader, class_names, dataset_name

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°
# ==========================================
def train_one_epoch(model, loader, criterion, optimizer, epoch):
    model.train()
    total_loss, total_correct = 0.0, 0
    bar = tqdm(loader, desc=f"Epoch {epoch}/{Config.EPOCHS} [Train]")
    
    for imgs, labels in bar:
        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)
        
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * imgs.size(0)
        total_correct += (outputs.argmax(1) == labels).sum().item()
        bar.set_postfix(loss=loss.item())
        
    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)

@torch.no_grad()
def validate(model, loader, criterion, epoch, phase="Val"):
    model.eval()
    total_loss, total_correct = 0.0, 0
    bar = tqdm(loader, desc=f"Epoch {epoch}/{Config.EPOCHS} [{phase}]  ")
    
    for imgs, labels in bar:
        imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        
        total_loss += loss.item() * imgs.size(0)
        total_correct += (outputs.argmax(1) == labels).sum().item()
        bar.set_postfix(loss=loss.item())
        
    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)

def evaluate_test_set(model, test_loader, class_names):
    if not test_loader or not HAS_SKLEARN: return
    print(f"\n[Test] æ‰§è¡Œæµ‹è¯•é›†è¯„ä¼°...")
    
    model.eval()
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for imgs, labels in tqdm(test_loader, desc="Testing"):
            imgs, labels = imgs.to(Config.DEVICE), labels.to(Config.DEVICE)
            outputs = model(imgs)
            all_preds.extend(outputs.argmax(1).cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è¾“å‡ºåˆ†ç±»æŠ¥å‘Š
    print("\n" + classification_report(all_labels, all_preds, target_names=class_names, digits=4))
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(10, 8))
    sns.heatmap(confusion_matrix(all_labels, all_preds), annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.savefig(os.path.join(Config.SAVE_DIR, 'confusion_matrix.png'))
    print(f"[Info] æ··æ·†çŸ©é˜µå·²ä¿å­˜ã€‚")

def plot_history(history, save_dir):
    epochs = range(1, len(history['train_acc']) + 1)
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_acc'], label='Train')
    plt.plot(epochs, history['val_acc'], label='Val')
    plt.legend(); plt.title('Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_loss'], label='Train')
    plt.plot(epochs, history['val_loss'], label='Val')
    plt.legend(); plt.title('Loss')
    
    plt.savefig(os.path.join(save_dir, 'training_curve.png'))
    print(f"[Info] æ›²çº¿å·²ä¿å­˜ã€‚")

def save_checkpoint(state, is_best, filename='last.pth'):
    """ä¿å­˜æ–­ç‚¹æ–‡ä»¶"""
    path = os.path.join(Config.SAVE_DIR, filename)
    torch.save(state, path)
    if is_best:
        torch.save(state, os.path.join(Config.SAVE_DIR, 'best_model.pth'))

# ==========================================
# 4. ä¸»ç¨‹åºå…¥å£
# ==========================================
if __name__ == "__main__":
    # --- 1. è·å–æ¨¡å‹é»˜è®¤é…ç½® ---
    temp_model = timm.create_model(Config.MODEL_NAME, pretrained=True)
    cfg = resolve_data_config({}, model=temp_model)
    del temp_model
    
    # --- 2. å‡†å¤‡æ•°æ® & ç”Ÿæˆä¿å­˜ç›®å½• ---
    train_tf, val_test_tf = get_transforms(cfg)
    train_loader, val_loader, test_loader, class_names, dataset_name = get_dataloaders(train_tf, val_test_tf)
    
    # ç¡®å®šä¿å­˜ç›®å½•é€»è¾‘
    if Config.RESUME_PATH:
        # å¦‚æœæ˜¯æ–­ç‚¹ç»­è®­ï¼Œå¤ç”¨åŸç›®å½•
        Config.SAVE_DIR = os.path.dirname(Config.RESUME_PATH)
        print(f"[Config] æ–­ç‚¹ç»­è®­æ¨¡å¼ï¼Œä½¿ç”¨åŸç›®å½•: {Config.SAVE_DIR}")
    else:
        # å¦‚æœæ˜¯æ–°è®­ç»ƒï¼Œç”Ÿæˆ "æ¨¡å‹_æ•°æ®é›†_æ—¶é—´" æ ¼å¼çš„ç›®å½•
        time_str = time.strftime("%Y%m%d_%H%M%S")
        run_name = f"{Config.MODEL_NAME}_{dataset_name}_{time_str}"
        Config.SAVE_DIR = os.path.join(Config.SAVE_DIR_ROOT, run_name)
        os.makedirs(Config.SAVE_DIR, exist_ok=True)
        print(f"[Config] å…¨æ–°è®­ç»ƒï¼Œä¿å­˜è‡³: {Config.SAVE_DIR}")

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ ---
    print(f"[Init] åˆ›å»ºæ¨¡å‹: {Config.MODEL_NAME}")
    
    # å¦‚æœæŒ‡å®šäº†æœ¬åœ°æƒé‡ä¸”ä¸æ˜¯ç»­è®­æ¨¡å¼ï¼Œåˆ™åŠ è½½æœ¬åœ°æ–‡ä»¶
    if not Config.RESUME_PATH and Config.CHECKPOINT_PATH and os.path.exists(Config.CHECKPOINT_PATH):
        print(f"[Load] åŠ è½½æœ¬åœ°åˆå§‹åŒ–æƒé‡: {Config.CHECKPOINT_PATH}")
        model = timm.create_model(Config.MODEL_NAME, pretrained=False, checkpoint_path=Config.CHECKPOINT_PATH)
    else:
        # å¦åˆ™ä½¿ç”¨åœ¨çº¿ä¸‹è½½çš„é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæ˜¯ç»­è®­ï¼Œç¨åä¼šè¢«è¦†ç›–ï¼‰
        model = timm.create_model(Config.MODEL_NAME, pretrained=True)
    
    # é‡ç½®åˆ†ç±»å¤´
    model.reset_classifier(num_classes=Config.NUM_CLASSES)
    model.to(Config.DEVICE)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=Config.LR, weight_decay=Config.WEIGHT_DECAY)
    
    # --- 4. æ–­ç‚¹æ¢å¤é€»è¾‘ ---
    start_epoch = 1
    best_acc = 0.0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    
    if Config.RESUME_PATH and os.path.exists(Config.RESUME_PATH):
        print(f"\n[Resume] æ­£åœ¨æ¢å¤æ–­ç‚¹: {Config.RESUME_PATH}")
        checkpoint = torch.load(Config.RESUME_PATH, map_location=Config.DEVICE)
        
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        start_epoch = checkpoint['epoch'] + 1
        best_acc = checkpoint['best_acc']
        history = checkpoint['history']
        
        print(f"[Resume] æ¢å¤æˆåŠŸ! ä»ç¬¬ {start_epoch} è½®ç»§ç»­ (å½“å‰æœ€ä½³: {best_acc:.4f})")
    
    # --- 5. è®­ç»ƒå¾ªç¯ ---
    print(f"\n[Start] å¼€å§‹è®­ç»ƒ... (è®¾å¤‡: {Config.DEVICE})")
    for epoch in range(start_epoch, Config.EPOCHS + 1):
        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, epoch)
        v_loss, v_acc = validate(model, val_loader, criterion, epoch)
        
        # æ›´æ–°å†å²è®°å½•
        history['train_loss'].append(t_loss); history['train_acc'].append(t_acc)
        history['val_loss'].append(v_loss); history['val_acc'].append(v_acc)
        
        print(f"Epoch {epoch}: Train Acc: {t_acc:.4f} | Val Acc: {v_acc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        is_best = v_acc > best_acc
        if is_best: 
            best_acc = v_acc
            print(f" -> ğŸŒŸ æ–°çš„æœ€ä½³æ¨¡å‹ (Acc: {best_acc:.4f})")
        
        # ä¿å­˜æ–­ç‚¹ (åŒ…å«æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€epochã€history)
        save_checkpoint({
            'epoch': epoch,
            'state_dict': model.state_dict(),
            'best_acc': best_acc,
            'optimizer': optimizer.state_dict(),
            'history': history,
        }, is_best, filename='last.pth')

    # --- 6. æ”¶å°¾å·¥ä½œ ---
    plot_history(history, Config.SAVE_DIR)
    
    if test_loader:
        print("[Info] åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
        # åŠ è½½ best_model.pth ä¸­çš„æƒé‡
        checkpoint = torch.load(os.path.join(Config.SAVE_DIR, "best_model.pth"), map_location=Config.DEVICE)
        model.load_state_dict(checkpoint['state_dict'])
        evaluate_test_set(model, test_loader, class_names)
        
    print("\n[Done] å…¨éƒ¨å®Œæˆï¼")
```